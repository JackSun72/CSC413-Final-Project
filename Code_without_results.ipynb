{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyzOT64xkqy6"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# CSC413 Final Project\n",
        "\n",
        "**Version Date**: 2023-04-07\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NE_CRp72_nR"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJfHM7NNru1e"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import math\n",
        "import time\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "import scipy.misc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import torchvision\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import argparse\n",
        "import math\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MNIST dataset & FCN"
      ],
      "metadata": {
        "id": "p8ZUSlNLJdie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MNIST_FullyConnected(nn.Module):\n",
        "    \"\"\"\n",
        "    A fully-connected NN for the MNIST task. This is Optimizable but not itself\n",
        "    an optimizer.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_inp, num_hid, num_out):\n",
        "        super(MNIST_FullyConnected, self).__init__()\n",
        "        self.layer1 = nn.Linear(num_inp, num_hid)\n",
        "        self.layer2 = nn.Linear(num_hid, num_out)\n",
        "\n",
        "    def initialize(self):\n",
        "        nn.init.kaiming_uniform_(self.layer1.weight, a=math.sqrt(5))\n",
        "        nn.init.kaiming_uniform_(self.layer2.weight, a=math.sqrt(5))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Compute a prediction.\"\"\"\n",
        "        x = self.layer1(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.layer2(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 5\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "mnist_train = torchvision.datasets.MNIST('./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
        "mnist_test = torchvision.datasets.MNIST('./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
        "dl_train = torch.utils.data.DataLoader(mnist_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dl_test = torch.utils.data.DataLoader(mnist_test, batch_size=10000, shuffle=False)"
      ],
      "metadata": {
        "id": "Qe-IwUz-KEWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperoptimizers by Chandra et al."
      ],
      "metadata": {
        "id": "4XYw5uWjJQek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Descent: The Ultimate Optimizer (Chandra et al.)\n",
        "class Optimizable:\n",
        "    '''\n",
        "    This is the interface for anything that has parameters that need to be\n",
        "    optimized, somewhat like torch.nn.Model but with the right plumbing for\n",
        "    hyperoptimizability. (Specifically, torch.nn.Model uses the Parameter\n",
        "    interface which does not give us enough control about the detachments.)\n",
        "    Nominal operation of an Optimizable at the lowest level is as follows:\n",
        "        o = MyOptimizable(...)\n",
        "        o.initialize()\n",
        "        loop {\n",
        "            o.begin()\n",
        "            o.zero_grad()\n",
        "            loss = --compute loss function from parameters--\n",
        "            loss.backward()\n",
        "            o.step()\n",
        "        }\n",
        "    Optimizables recursively handle updates to their optimiz*ers*.\n",
        "    '''\n",
        "    def __init__(self, parameters, optimizer):\n",
        "        self.parameters = parameters # a dict mapping names to tensors\n",
        "        self.optimizer = optimizer   # which must itself be Optimizable!\n",
        "        self.all_params_with_gradients = []\n",
        "\n",
        "    def initialize(self):\n",
        "        ''' Initialize parameters, e.g. with a Kaiming initializer. '''\n",
        "        pass\n",
        "    \n",
        "    def begin(self):\n",
        "        ''' Enable gradient tracking on current parameters. '''\n",
        "        for param in self.all_params_with_gradients:\n",
        "             param.grad = None\n",
        "        self.all_params_with_gradients.clear()\n",
        "        for name, param in self.parameters.items():\n",
        "            param.requires_grad_() # keep gradient information...\n",
        "            param.retain_grad()    # even if not a leaf...\n",
        "            self.all_params_with_gradients.append(param)\n",
        "        self.optimizer.begin()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        ''' Set all gradients to zero. '''\n",
        "        for param in self.all_params_with_gradients:\n",
        "            param.grad = torch.zeros_like(param)\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    ''' Note: at this point you would probably call .backwards() on the loss\n",
        "    function. '''\n",
        "\n",
        "    def step(self):\n",
        "        ''' Update parameters '''\n",
        "        pass\n",
        "\n",
        "class NoOpOptimizer(Optimizable):\n",
        "    '''\n",
        "    NoOpOptimizer sits on top of a stack, and does not affect what lies below.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def initialize(self):\n",
        "        pass\n",
        "\n",
        "    def begin(self):\n",
        "        pass\n",
        "\n",
        "    def zero_grad(self):\n",
        "        pass\n",
        "\n",
        "    def step(self, params):\n",
        "        pass\n",
        "\n",
        "    def __str__(self):\n",
        "        return ''\n",
        "\n",
        "class SGD(Optimizable):\n",
        "    '''\n",
        "    A hyperoptimizable SGD.\n",
        "    '''\n",
        "    def __init__(self, alpha=0.01, mu=0.0, optimizer=NoOpOptimizer()):\n",
        "        self.mu = mu\n",
        "        self.state = {}\n",
        "        parameters = {\n",
        "            'alpha': torch.tensor(alpha),\n",
        "            'mu': torch.tensor(mu)\n",
        "        }\n",
        "        super().__init__(parameters, optimizer)\n",
        "\n",
        "    def step(self, params):\n",
        "        self.optimizer.step(self.parameters)\n",
        "        for name, param in params.items():\n",
        "            g = param.grad.detach()\n",
        "            p = param.detach()\n",
        "            if self.mu != 0.0:\n",
        "                if name not in self.state:\n",
        "                    buf = self.state[name] = g\n",
        "                else:\n",
        "                    buf = self.state[name].detach()\n",
        "                    buf = buf * self.parameters['mu'] + g\n",
        "                g = self.state[name] = buf\n",
        "            params[name] = p - g * self.parameters['alpha']\n",
        "        \n",
        "    def __str__(self):\n",
        "        return 'sgd / '+ str(self.optimizer)\n",
        "\n",
        "class SGDPerParam(Optimizable):\n",
        "    '''\n",
        "    Optimizes parameters individually with SGD.\n",
        "    '''\n",
        "    def __init__(self, params, optimizer=NoOpOptimizer()):\n",
        "        parameters = {k + '_alpha' : torch.tensor(v) for k, v in params}\n",
        "        super().__init__(parameters, optimizer)\n",
        "\n",
        "    def step(self, params):\n",
        "        self.optimizer.step(self.parameters)\n",
        "        for name, param in params.items():\n",
        "            g = param.grad.detach()\n",
        "            p = param.detach()\n",
        "            if name + '_alpha' not in self.parameters: params[name] = p\n",
        "            else: params[name] = p - g * self.parameters[name + '_alpha']\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'sgdPerParam / ' + str(self.optimizer)\n",
        "\n",
        "class AdaGrad(Optimizable):\n",
        "    '''\n",
        "    A hyperoptimizable AdaGrad.\n",
        "    '''\n",
        "    def __init__(self, alpha=0.01, optimizer=NoOpOptimizer()):\n",
        "        self.eps = 1e-10\n",
        "        self.cache = {}\n",
        "        parameters = {\n",
        "            'alpha': torch.tensor(alpha)\n",
        "        }\n",
        "        super().__init__(parameters, optimizer)\n",
        "    \n",
        "    def step(self, params):\n",
        "        self.optimizer.step(self.parameters)\n",
        "        for name, param in params.items():\n",
        "            if name not in self.cache:\n",
        "                self.cache[name] = {\n",
        "                    'G': torch.zeros_like(param) + 1e-1\n",
        "                }\n",
        "            g = param.grad.detach()\n",
        "            self.cache[name]['G'] = G = self.cache[name]['G'].detach() + torch.square(g)\n",
        "            params[name] = param.detach() - self.parameters['alpha'] * g / torch.sqrt(G + self.eps).detach()\n",
        "    \n",
        "    def __str__(self):\n",
        "        return 'adagrad / ' + str(self.optimizer)\n",
        "\n",
        "class RMSProp(Optimizable):\n",
        "    '''\n",
        "    A hyperoptimizable RMSProp.\n",
        "    '''\n",
        "    def clamp(x):\n",
        "        return (x.tanh() + 1.) / 2.\n",
        "\n",
        "    def unclamp(y):\n",
        "        z = y * 2. - 1.\n",
        "        return ((1. + z) / (1. - z)).log() / 2.\n",
        "\n",
        "    def __init__(self, alpha=0.01, gamma=0.99, optimizer=NoOpOptimizer()):\n",
        "        self.eps = 1e-8\n",
        "        parameters = {\n",
        "            'alpha': torch.sqrt(torch.tensor(alpha)),\n",
        "            'gamma': RMSProp.unclamp(torch.tensor(gamma))\n",
        "        }\n",
        "        super().__init__(parameters, optimizer)\n",
        "        self.cache = {}\n",
        "\n",
        "    def step(self, params):\n",
        "        self.optimizer.step(self.parameters)\n",
        "        gamma = RMSProp.clamp(self.parameters['gamma'])\n",
        "        alpha = torch.square(self.parameters['alpha'])\n",
        "        for name, param in params.items():\n",
        "            if name not in self.cache:\n",
        "                self.cache[name] = {\n",
        "                    's': torch.zeros_like(param)\n",
        "                }\n",
        "            g = param.grad.detach()\n",
        "            self.cache[name]['s'] = s = gamma * self.cache[name]['s'].detach() + (1. - gamma) * torch.square(g)\n",
        "            self.all_params_with_gradients.append(s)\n",
        "            params[name] = param.detach() - alpha * g / torch.sqrt(s + self.eps)\n",
        "    \n",
        "    def __str__(self):\n",
        "        return 'rmsprop / ' + str(self.optimizer)\n",
        "\n",
        "class RMSPropAlpha(Optimizable):\n",
        "    '''\n",
        "    A hyperoptimizable RMSProp for only alpha.\n",
        "    '''\n",
        "    def __init__(self, alpha=0.01, gamma=0.99, optimizer=NoOpOptimizer()):\n",
        "        self.eps = 1e-8\n",
        "        self.gamma = gamma\n",
        "        parameters = {\n",
        "            'alpha': torch.sqrt(torch.tensor(alpha)),\n",
        "        }\n",
        "        super().__init__(parameters, optimizer)\n",
        "        self.cache = {}\n",
        "\n",
        "    def step(self, params):\n",
        "        self.optimizer.step(self.parameters)\n",
        "        alpha = torch.square(self.parameters['alpha'])\n",
        "        for name, param in params.items():\n",
        "            if name not in self.cache:\n",
        "                self.cache[name] = {\n",
        "                    's': torch.zeros_like(param)\n",
        "                }\n",
        "            g = param.grad.detach()\n",
        "            self.cache[name]['s'] = s = self.gamma * self.cache[name]['s'].detach() + (1. - self.gamma) * torch.square(g)\n",
        "            self.all_params_with_gradients.append(s)\n",
        "            params[name] = param.detach() - alpha * g / torch.sqrt(s + self.eps)\n",
        "    \n",
        "    def __str__(self):\n",
        "        return 'rmspropAlpha / ' + str(self.optimizer)\n",
        "\n",
        "class Adam(Optimizable):\n",
        "    '''\n",
        "    A hyperoptimizable Adam optimizer.\n",
        "    '''\n",
        "    def clamp(x):\n",
        "        return (x.tanh() + 1.) / 2.\n",
        "\n",
        "    def unclamp(y):\n",
        "        z = y * 2. - 1.\n",
        "        return ((1. + z) / (1. - z)).log() / 2.\n",
        "\n",
        "    def __init__(self, alpha=0.001, beta1=0.9, beta2=0.999, log_eps=-8., optimizer=NoOpOptimizer()):\n",
        "        self.eps = 10. ** log_eps\n",
        "        parameters = {\n",
        "            'alpha': torch.tensor(alpha),\n",
        "            'beta1': Adam.unclamp(torch.tensor(beta1)),\n",
        "            'beta2': Adam.unclamp(torch.tensor(beta2)),\n",
        "        }\n",
        "        super().__init__(parameters, optimizer)\n",
        "        self.num_stepments = 0\n",
        "        self.cache = {}\n",
        "\n",
        "    def step(self, params):\n",
        "        self.num_stepments += 1\n",
        "        self.optimizer.step(self.parameters)\n",
        "        t = self.num_stepments\n",
        "        beta1 = Adam.clamp(self.parameters['beta1'])\n",
        "        beta2 = Adam.clamp(self.parameters['beta2'])\n",
        "        for name, param in params.items():\n",
        "            if name not in self.cache:\n",
        "                self.cache[name] = {\n",
        "                    'm': torch.zeros_like(param),\n",
        "                    'v': torch.zeros_like(param) +\\\n",
        "                            self.eps\n",
        "# NOTE that we add a little `fudge factor' here because sqrt is not\n",
        "# differentiable at exactly zero\n",
        "                }\n",
        "            g = param.grad.detach()\n",
        "            self.cache[name]['m'] = m =\\\n",
        "                beta1 * self.cache[name]['m'].detach() + (1. - beta1) * g\n",
        "            self.cache[name]['v'] = v =\\\n",
        "                beta2 * self.cache[name]['v'].detach() + (1. - beta2) * g * g\n",
        "            self.all_params_with_gradients.append(m)\n",
        "            self.all_params_with_gradients.append(v)\n",
        "\n",
        "            m_hat = m / (1. - beta1 ** float(t))\n",
        "            v_hat = v / (1. - beta2 ** float(t))\n",
        "\n",
        "            dparam = m_hat / (v_hat ** 0.5 + self.eps)\n",
        "            params[name] = param.detach() - self.parameters['alpha'] * dparam\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'adam / ' + str(self.optimizer)\n",
        "\n",
        "class AdamBaydin(Optimizable):\n",
        "    ''' Same as above, but only optimizes the learning rate, treating the\n",
        "    remaining hyperparameters as constants. '''\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        alpha=0.001, beta1=0.9, beta2=0.999, log_eps=-8.,\n",
        "        optimizer=NoOpOptimizer()\n",
        "    ):\n",
        "        parameters = {\n",
        "            'alpha': torch.tensor(alpha),\n",
        "        }\n",
        "        self.alpha = alpha\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.log_eps = log_eps\n",
        "        super().__init__(parameters, optimizer)\n",
        "        self.num_stepments = 0\n",
        "        self.cache = {}\n",
        "\n",
        "    def step(self, params):\n",
        "        self.num_stepments += 1\n",
        "        self.optimizer.step(self.parameters)\n",
        "        t = self.num_stepments\n",
        "        beta1 = self.beta1\n",
        "        beta2 = self.beta2\n",
        "        for name, param in params.items():\n",
        "            if name not in self.cache:\n",
        "                self.cache[name] = {\n",
        "                    'm': torch.zeros_like(param),\n",
        "                    'v': torch.zeros_like(param) +\\\n",
        "                            10.**self.log_eps\n",
        "# NOTE that we add a little `fudge factor' here because sqrt is not\n",
        "# differentiable at exactly zero\n",
        "                }\n",
        "\n",
        "            g = param.grad.detach()\n",
        "            self.cache[name]['m'] = m =\\\n",
        "                beta1 * self.cache[name]['m'].detach() + (1. - beta1) * g\n",
        "            self.cache[name]['v'] = v =\\\n",
        "                beta2 * self.cache[name]['v'].detach() + (1. - beta2) * g * g\n",
        "\n",
        "            self.all_params_with_gradients.append(m)\n",
        "            self.all_params_with_gradients.append(v)\n",
        "\n",
        "            m_hat = m / (1. - beta1 ** float(t))\n",
        "            v_hat = v / (1. - beta2 ** float(t))\n",
        "\n",
        "            dparam = m_hat / (v_hat ** 0.5 + 10. ** self.log_eps)\n",
        "            params[name] = param.detach() - self.parameters['alpha'] * dparam\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'adamBaydin / ' + str(self.optimizer)\n",
        "\n",
        "\n",
        "class ModuleWrapper(Optimizable):\n",
        "    '''\n",
        "    This class tries to convert a torch.nn.Module to an Optimizable, handling\n",
        "    the internal plumbing needed to update parameters correctly.\n",
        "    '''\n",
        "    def __init__(self, module, optimizer=NoOpOptimizer()):\n",
        "        self.module = module\n",
        "        parameters = {k:v for k, v in module.named_parameters(recurse=True)}\n",
        "        super().__init__(parameters, optimizer)\n",
        "    \n",
        "    def initialize(self):\n",
        "        self.optimizer.initialize()\n",
        "    \n",
        "    def zero_grad(self):\n",
        "        \"\"\" Set all gradients to zero. \"\"\"\n",
        "        self.module.zero_grad()\n",
        "        for param in self.all_params_with_gradients:\n",
        "            param.grad = torch.zeros_like(param)\n",
        "        self.optimizer.zero_grad()\n",
        "    \n",
        "    def forward(self, *xyz):\n",
        "        return self.module(*xyz)\n",
        "    \n",
        "    def train(self):\n",
        "        self.module.train()\n",
        "    \n",
        "    def eval(self):\n",
        "        self.module.eval()\n",
        "    \n",
        "    def step(self):\n",
        "        self.optimizer.step(self.parameters)\n",
        "        def set_param(m, k, v):\n",
        "            kk = k\n",
        "            while '.' in k:\n",
        "                sm = k[:k.index('.')]\n",
        "                k = k[k.index('.') + 1:]\n",
        "                m = m._modules[sm]\n",
        "\n",
        "            m._parameters[k] = None\n",
        "            m._parameters[k] = self.parameters[kk]\n",
        "\n",
        "        for k, v in self.module.named_parameters(recurse=True):\n",
        "            set_param(self.module, k, v)"
      ],
      "metadata": {
        "id": "p2FiXylnKLG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### New hyperoptimizers for AdaBelief and AMSProp"
      ],
      "metadata": {
        "id": "JdKDw3XcJnDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adabelief\n",
        "class AdaBelief(Optimizable):\n",
        "    '''\n",
        "    A hyperoptimizable Adabelief optimizer.\n",
        "    '''\n",
        "    def clamp(x):\n",
        "        return (x.tanh() + 1.) / 2.\n",
        "\n",
        "    def unclamp(y):\n",
        "        z = y * 2. - 1.\n",
        "        return ((1. + z) / (1. - z)).log() / 2.\n",
        "\n",
        "    def __init__(self, alpha=0.001, beta1=0.9, beta2=0.999, log_eps=-8., optimizer=NoOpOptimizer()):\n",
        "        self.eps = 10. ** log_eps\n",
        "        parameters = {\n",
        "            'alpha': torch.tensor(alpha),\n",
        "            'beta1': Adam.unclamp(torch.tensor(beta1)),\n",
        "            'beta2': Adam.unclamp(torch.tensor(beta2)),\n",
        "        }\n",
        "        super().__init__(parameters, optimizer)\n",
        "        self.num_stepments = 0\n",
        "        self.cache = {}\n",
        "\n",
        "    def step(self, params):\n",
        "        self.num_stepments += 1\n",
        "        self.optimizer.step(self.parameters)\n",
        "        t = self.num_stepments\n",
        "        beta1 = Adam.clamp(self.parameters['beta1'])\n",
        "        beta2 = Adam.clamp(self.parameters['beta2'])\n",
        "        for name, param in params.items():\n",
        "            if name not in self.cache:\n",
        "                self.cache[name] = {\n",
        "                    'm': torch.zeros_like(param),\n",
        "                    's': torch.zeros_like(param) +\\\n",
        "                            self.eps\n",
        "# NOTE that we add a little `fudge factor' here because sqrt is not\n",
        "# differentiable at exactly zero\n",
        "                }\n",
        "            g = param.grad.detach()\n",
        "            m = beta1 * self.cache[name]['m'].detach() + (1. - beta1) * g\n",
        "            self.cache[name]['s'] = s  =\\\n",
        "                beta2 * self.cache[name]['s'].detach() + (1. - beta2) * (\n",
        "                    g - m) ** 2 +  self.eps\n",
        "            self.cache[name]['m'] = m\n",
        "            self.all_params_with_gradients.append(m)\n",
        "            self.all_params_with_gradients.append(s)\n",
        "\n",
        "            m_hat = m / (1. - beta1 ** float(t))\n",
        "            s_hat = s / (1. - beta2 ** float(t))\n",
        "\n",
        "            dparam = m_hat / (s_hat ** 0.5 + self.eps)\n",
        "            params[name] = param.detach() - self.parameters['alpha'] * dparam\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'adabelief / ' + str(self.optimizer)\n",
        "\n",
        "\n",
        "class AdaBeliefBaydin(Optimizable):\n",
        "    '''\n",
        "    A hyperoptimizable AdabeliefBaydin optimizer.\n",
        "    '''\n",
        "    def __init__(self, alpha=0.001, beta1=0.9, beta2=0.999, log_eps=-8., optimizer=NoOpOptimizer()):\n",
        "        self.eps = 10. ** log_eps\n",
        "        parameters = {\n",
        "            'alpha': torch.tensor(alpha),\n",
        "        }\n",
        "        self.alpha = alpha\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.log_eps = log_eps\n",
        "        super().__init__(parameters, optimizer)\n",
        "        self.num_stepments = 0\n",
        "        self.cache = {}\n",
        "\n",
        "    def step(self, params):\n",
        "        self.num_stepments += 1\n",
        "        self.optimizer.step(self.parameters)\n",
        "        t = self.num_stepments\n",
        "        beta1 = self.beta1\n",
        "        beta2 = self.beta2\n",
        "        for name, param in params.items():\n",
        "            if name not in self.cache:\n",
        "                self.cache[name] = {\n",
        "                    'm': torch.zeros_like(param),\n",
        "                    's': torch.zeros_like(param) +\\\n",
        "                            self.eps\n",
        "# NOTE that we add a little `fudge factor' here because sqrt is not\n",
        "# differentiable at exactly zero\n",
        "                }\n",
        "            g = param.grad.detach()\n",
        "            m = beta1 * self.cache[name]['m'].detach() + (1. - beta1) * g\n",
        "            self.cache[name]['s'] = s  =\\\n",
        "                beta2 * self.cache[name]['s'].detach() + (1. - beta2) * (\n",
        "                    g - m) ** 2 +  self.eps\n",
        "            self.cache[name]['m'] = m\n",
        "            self.all_params_with_gradients.append(m)\n",
        "            self.all_params_with_gradients.append(s)\n",
        "\n",
        "            m_hat = m / (1. - beta1 ** float(t))\n",
        "            s_hat = s / (1. - beta2 ** float(t))\n",
        "\n",
        "            dparam = m_hat / (s_hat ** 0.5 + self.eps)\n",
        "            params[name] = param.detach() - self.parameters['alpha'] * dparam\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'adabeliefbaydin / ' + str(self.optimizer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Amsgrad\n",
        "class AMSGrad(Optimizable):\n",
        "    '''\n",
        "    A hyperoptimizable AMSGrad optimizer.\n",
        "    '''\n",
        "    def clamp(x):\n",
        "        return (x.tanh() + 1.) / 2.\n",
        "\n",
        "    def unclamp(y):\n",
        "        z = y * 2. - 1.\n",
        "        return ((1. + z) / (1. - z)).log() / 2.\n",
        "\n",
        "    def __init__(self, alpha=0.001, beta1=0.9, beta2=0.999, log_eps=-8., optimizer=NoOpOptimizer()):\n",
        "        self.eps = 10. ** log_eps\n",
        "        parameters = {\n",
        "            'alpha': torch.tensor(alpha),\n",
        "            'beta1': Adam.unclamp(torch.tensor(beta1)),\n",
        "            'beta2': Adam.unclamp(torch.tensor(beta2)),\n",
        "        }\n",
        "        super().__init__(parameters, optimizer)\n",
        "        self.num_stepments = 0\n",
        "        self.cache = {}\n",
        "\n",
        "    def step(self, params):\n",
        "        self.num_stepments += 1\n",
        "        self.optimizer.step(self.parameters)\n",
        "        t = self.num_stepments\n",
        "        beta1 = Adam.clamp(self.parameters['beta1'])\n",
        "        beta2 = Adam.clamp(self.parameters['beta2'])\n",
        "        for name, param in params.items():\n",
        "            if name not in self.cache:\n",
        "                self.cache[name] = {\n",
        "                    'm': torch.zeros_like(param),\n",
        "                    'v': torch.zeros_like(param) +\\\n",
        "                            self.eps\n",
        "                }\n",
        "            g = param.grad.detach()\n",
        "            prev_v = self.cache[name]['v'].detach()\n",
        "            self.cache[name]['m'] = m =\\\n",
        "                beta1 * self.cache[name]['m'].detach() + (1. - beta1) * g\n",
        "\n",
        "            v = beta2 * prev_v + (1. - beta2) * g * g\n",
        "\n",
        "            self.cache[name]['v'] = v\n",
        "            self.all_params_with_gradients.append(m)\n",
        "            self.all_params_with_gradients.append(v)\n",
        "\n",
        "            v_hat = torch.maximum(prev_v, v)\n",
        "\n",
        "            dparam = m / (v_hat ** 0.5 + self.eps)\n",
        "            params[name] = param.detach() - self.parameters['alpha'] * dparam\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'amsgrad / ' + str(self.optimizer)\n",
        "  \n",
        "\n",
        "class AMSGradBaydin(Optimizable):\n",
        "    '''\n",
        "    A hyperoptimizable AMSGradBaydin optimizer.\n",
        "    '''\n",
        "    def clamp(x):\n",
        "        return (x.tanh() + 1.) / 2.\n",
        "\n",
        "    def unclamp(y):\n",
        "        z = y * 2. - 1.\n",
        "        return ((1. + z) / (1. - z)).log() / 2.\n",
        "\n",
        "    def __init__(self, alpha=0.001, beta1=0.9, beta2=0.999, log_eps=-8., optimizer=NoOpOptimizer()):\n",
        "\n",
        "\n",
        "        self.eps = 10. ** log_eps\n",
        "        parameters = {\n",
        "            'alpha': torch.tensor(alpha),\n",
        "        }\n",
        "        self.alpha = alpha\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.log_eps = log_eps\n",
        "        super().__init__(parameters, optimizer)\n",
        "        self.num_stepments = 0\n",
        "        self.cache = {}\n",
        "\n",
        "    def step(self, params):\n",
        "        self.num_stepments += 1\n",
        "        self.optimizer.step(self.parameters)\n",
        "        t = self.num_stepments\n",
        "        beta1 =self.beta1\n",
        "        beta2 = self.beta2\n",
        "        for name, param in params.items():\n",
        "            if name not in self.cache:\n",
        "                self.cache[name] = {\n",
        "                    'm': torch.zeros_like(param),\n",
        "                    'v': torch.zeros_like(param) +\\\n",
        "                            self.eps\n",
        "                }\n",
        "            g = param.grad.detach()\n",
        "            prev_v = self.cache[name]['v'].detach()\n",
        "            self.cache[name]['m'] = m =\\\n",
        "                beta1 * self.cache[name]['m'].detach() + (1. - beta1) * g\n",
        "\n",
        "            v = beta2 * prev_v + (1. - beta2) * g * g\n",
        "\n",
        "            self.cache[name]['v'] = v\n",
        "            self.all_params_with_gradients.append(m)\n",
        "            self.all_params_with_gradients.append(v)\n",
        "\n",
        "            v_hat = torch.maximum(prev_v, v)\n",
        "\n",
        "            dparam = m / (v_hat ** 0.5 + self.eps)\n",
        "            params[name] = param.detach() - self.parameters['alpha'] * dparam\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'amsgrad / ' + str(self.optimizer)"
      ],
      "metadata": {
        "id": "ge4sIjzHKRxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iti2Tnjxxwqn"
      },
      "source": [
        "## Part 1: Reproduction of the original Paper\n",
        "In this part, we will compare the performance of classical optimizers and hyperoptimized optimizers on MNIST dataset. Specifically, we will compare the losses and accuracies of:\n",
        "\n",
        "\n",
        "1. \n",
        "Classical SGD v.s. SGD with SGD/Adam/AdaGrad/RMSProp hyperoptimizers\n",
        "\n",
        "2. Classical Adam v.s. Adam with Adam/SGD hyperoptimizers.\n",
        "\n",
        "3. AdaGrad v.s. AdaGrad with SGD/AdaGrad hyperoptimizers.\n",
        "\n",
        "4. RMSProp v.s. SGD and RMSProp optimizers. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r6e6jBC3NgF"
      },
      "source": [
        "#### Training and generating optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoUfrZNYPZhi"
      },
      "outputs": [],
      "source": [
        "def train(model, mw):\n",
        "  # Initialize lists to store losses\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "  accuracy = []\n",
        "  final_parameters={}\n",
        "  total_time = 0\n",
        "  # Training loop\n",
        "  for i in range(1, EPOCHS+1):\n",
        "    running_loss = 0.0\n",
        "    start_time = time.time()\n",
        "    for j, (features_, labels_) in enumerate(dl_train):\n",
        "        mw.begin() # call this before each step, enables gradient tracking on desired params\n",
        "        features, labels = torch.reshape(features_, (-1, 28 * 28)).to(DEVICE), labels_.to(DEVICE)\n",
        "        pred = mw.forward(features)\n",
        "        loss = F.nll_loss(pred, labels)\n",
        "        mw.zero_grad()\n",
        "        loss.backward(create_graph=True) # important! use create_graph=True\n",
        "        mw.step()\n",
        "        running_loss += loss.item() * features_.size(0)\n",
        "    end_time = time.time()\n",
        "    total_time += (end_time-start_time)\n",
        "    train_loss = running_loss / len(dl_train.dataset)\n",
        "    train_losses.append(train_loss)\n",
        "    # Evaluate on test dataset\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in dl_test:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            data = data.view(data.size(0), -1)  # Flatten the input data\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # Sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(dl_test.dataset)\n",
        "    val_losses.append(test_loss)\n",
        "    accuracy.append(100. * correct / len(dl_test.dataset))\n",
        "    print('Epoch: {}, Train loss: {:.4f}, Test loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "          i, train_loss, test_loss, correct, len(dl_test.dataset),\n",
        "          100. * correct / len(dl_test.dataset)))\n",
        "\n",
        "  print(\"The training time is: \"+ str(total_time))\n",
        "  for i in mw.optimizer.parameters:\n",
        "    if ((type(mw.optimizer)==Adam or type(mw.optimizer)==AdamBaydin or type(mw.optimizer)==AdaBeliefBaydin or type(mw.optimizer)==AdaBelief or type(mw.optimizer)==AMSGradBaydin or type(mw.optimizer)==AMSGrad) \n",
        "    and (i in {'beta1','beta2'})) or ((type(mw.optimizer)==RMSProp or type(mw.optimizer)==RMSPropAlpha) and i in {'gamma'}):\n",
        "      final_parameters[i] = Adam.clamp(mw.optimizer.parameters[i]).item()\n",
        "    elif ((type(mw.optimizer)==RMSProp or type(mw.optimizer)==RMSPropAlpha) and i in {'alpha'}):\n",
        "      final_parameters[i] = mw.optimizer.parameters[i].item()**2\n",
        "    else:\n",
        "      final_parameters[i] = mw.optimizer.parameters[i].item()\n",
        "    print(i + \": \" +str(final_parameters[i]))\n",
        "  return train_losses, val_losses, accuracy, final_parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOxKF6g96pXI"
      },
      "outputs": [],
      "source": [
        "def generate_optimizers(optimizer1, hyperoptimizer1=False, lr=False):\n",
        "\n",
        "  dict1 = {'Adam': Adam(lr), 'AdaGrad': AdaGrad(lr), \n",
        "           'RMSProp': RMSProp(lr), 'SGD': SGD(lr), 'AdaBelief': AdaBelief(lr), \n",
        "           'AMSGrad': AMSGrad(lr), False: NoOpOptimizer()}\n",
        "  model = MNIST_FullyConnected(28 * 28, 128, 10).to(DEVICE)\n",
        "  \n",
        "  if optimizer1=='Adam':\n",
        "    opt = Adam(optimizer=dict1[hyperoptimizer1])\n",
        "  if optimizer1=='AdaGrad':\n",
        "    opt = AdaGrad(optimizer=dict1[hyperoptimizer1])\n",
        "  if optimizer1=='RMSProp':\n",
        "    opt = RMSProp(optimizer=dict1[hyperoptimizer1])\n",
        "  if optimizer1=='SGD':\n",
        "    opt = SGD(optimizer=dict1[hyperoptimizer1])\n",
        "  if optimizer1=='AdamBaydin':\n",
        "    opt = AdamBaydin(optimizer=dict1[hyperoptimizer1])\n",
        "  if optimizer1=='RMSPropAlpha':\n",
        "    opt = RMSPropAlpha(optimizer=dict1[hyperoptimizer1])\n",
        "  if optimizer1 == 'AdaBelief':\n",
        "    opt = AdaBelief(optimizer=dict1[hyperoptimizer1])\n",
        "  if optimizer1 == 'AMSGrad':\n",
        "    opt = AMSGrad(optimizer=dict1[hyperoptimizer1])\n",
        "\n",
        "  mw = ModuleWrapper(model, optimizer=opt)\n",
        "  return model, mw\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnuFAOQ96E2t"
      },
      "source": [
        "### Train by classical and hyperoptimized optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppHDG0CqB5A6"
      },
      "source": [
        "#### SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9-S7FVeB4Gi"
      },
      "outputs": [],
      "source": [
        "# SGD:\n",
        "model, mw = generate_optimizers('SGD')\n",
        "SGD_train_losses, SGD_val_losses, SGD_accuracy, _ = train(model, mw)\n",
        "model, mw = generate_optimizers('SGD','SGD',0.01)\n",
        "SGD_SGD_train_losses, SGD_SGD_val_losses, SGD_SGD_accuracy, SGD_SGD_params = train(model, mw)\n",
        "model, mw = generate_optimizers('SGD','Adam',0.1)\n",
        "SGD_Adam_train_losses, SGD_Adam_val_losses, SGD_Adam_accuracy, SGD_Adam_params = train(model, mw)\n",
        "model, mw = generate_optimizers('SGD','AdaGrad',0.01)\n",
        "SGD_AdaGrad_train_losses, SGD_AdaGrad_val_losses, SGD_AdaGrad_accuracy, SGD_AdaGrad_params = train(model, mw)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the actual 5th run\n",
        "model, mw = generate_optimizers('SGD','RMSProp',0.1)\n",
        "SGD_RMSProp_train_losses, SGD_RMSProp_val_losses, SGD_RMSProp_accuracy, SGD_RMSProp_params = train(model, mw)\n"
      ],
      "metadata": {
        "id": "_n86DYOxuocm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aoAhXhVCLBN"
      },
      "source": [
        "#### Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXD3KoWEgmPc"
      },
      "outputs": [],
      "source": [
        "# the printed beta is weird, i.e. about 3, but you can do the clamp/unclamp operation to modify it.\n",
        "\n",
        "\n",
        "model, mw = generate_optimizers('Adam')\n",
        "Adam_train_losses, Adam_val_losses, Adam_accuracy, _ = train(model, mw)\n",
        "model, mw = generate_optimizers('Adam','SGD',0.00001)\n",
        "Adam_SGD_train_losses, Adam_SGD_val_losses, Adam_SGD_accuracy, Adam_SGD_params = train(model, mw)\n",
        "model, mw = generate_optimizers('AdamBaydin','SGD',0.00001)\n",
        "AdamBaydin_SGD_train_losses, AdamBaydin_SGD_val_losses, AdamBaydin_SGD_accuracy, AdamBaydin_SGD_params = train(model, mw)\n",
        "model, mw = generate_optimizers('Adam','Adam',0.001)\n",
        "Adam_Adam_train_losses, Adam_Adam_val_losses, Adam_Adam_accuracy, Adam_Adam_params = train(model, mw)\n",
        "model, mw = generate_optimizers('AdamBaydin','Adam',0.001)\n",
        "AdamBaydin_Adam_train_losses, AdamBaydin_Adam_val_losses, AdamBaydin_Adam_accuracy, AdamBaydin_Adam_params = train(model, mw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOJt4xjfC6Pn"
      },
      "source": [
        "#### AdaGrad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yr9oFgfC5gu"
      },
      "outputs": [],
      "source": [
        "model, mw = generate_optimizers('AdaGrad')\n",
        "AdaGrad_train_losses, AdaGrad_val_losses,AdaGrad_accuracy, _ = train(model, mw)\n",
        "model, mw = generate_optimizers('AdaGrad','SGD',0.01)\n",
        "AdaGrad_SGD_train_losses, AdaGrad_SGD_val_losses, AdaGrad_SGD_accuracy, AdaGrad_SGD_params = train(model, mw)\n",
        "model, mw = generate_optimizers('AdaGrad','AdaGrad',0.01)\n",
        "AdaGrad_AdaGrad_train_losses, AdaGrad_AdaGrad_val_losses, AdaGrad_AdaGrad_accuracy, AdaGrad_AdaGrad_params = train(model, mw)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkz_F_FlDQ-S"
      },
      "source": [
        "#### RMSProp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3lL2TthDSUx"
      },
      "outputs": [],
      "source": [
        "EPOCHS=5\n",
        "model, mw = generate_optimizers('RMSProp')\n",
        "RMSProp_train_losses, RMSProp_val_losses,RMSProp_accuracy, _ = train(model, mw)\n",
        "\n",
        "model, mw = generate_optimizers('RMSPropAlpha','SGD',0.0001)\n",
        "RMSPropAlpha_SGD_train_losses, RMSPropAlpha_SGD_val_losses, RMSPropAlpha_SGD_accuracy, RMSPropAlpha_SGD_params = train(model, mw)\n",
        "model, mw = generate_optimizers('RMSProp','SGD',0.0001)\n",
        "RMSProp_SGD_train_losses, RMSProp_SGD_val_losses, RMSProp_SGD_accuracy, RMSProp_SGD_params = train(model, mw)\n",
        "model, mw = generate_optimizers('RMSPropAlpha','RMSProp',0.0001)\n",
        "RMSPropAlpha_RMSProp_train_losses, RMSPropAlpha_RMSProp_val_losses, RMSPropAlpha_RMSProp_accuracy, RMSPropAlpha_RMSProp_params = train(model, mw)\n",
        "model, mw = generate_optimizers('RMSProp','RMSProp',0.0001)\n",
        "RMSProp_RMSProp_train_losses, RMSProp_RMSProp_val_losses, RMSProp_RMSProp_accuracy, RMSProp_RMSProp_params = train(model, mw)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AdaBelief"
      ],
      "metadata": {
        "id": "MbwRBO0oK2Za"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e9lLiEdxfJr"
      },
      "outputs": [],
      "source": [
        "# AdaBelief:\n",
        "EPOCHS=5\n",
        "model, mw = generate_optimizers('AdaBelief')\n",
        "AdaBelief_train_losses, AdaBelief_val_losses, AdaBelief_accuracy, _ = train(model, mw)\n",
        "model, mw = generate_optimizers('AdaBelief','SGD',0.0001)\n",
        "AdaBelief_SGD_train_losses, AdaBelief_SGD_val_losses, AdaBelief_SGD_accuracy, AdaBelief_SGD_params = train(model, mw)\n",
        "model, mw = generate_optimizers('AdaBelief','Adam',0.0001)\n",
        "AdaBelief_Adam_train_losses, AdaBelief_Adam_val_losses, AdaBelief_Adam_accuracy, AdaBelief_Adam_params = train(model, mw)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AMSGrad"
      ],
      "metadata": {
        "id": "JWwyV8sGK4eO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtSr8UqMxhSl"
      },
      "outputs": [],
      "source": [
        "# AMSGrad:\n",
        "model, mw = generate_optimizers('AMSGrad')\n",
        "AMSGrad_train_losses, AMSGrad_val_losses, AMSGrad_accuracy, _ = train(model, mw)\n",
        "model, mw = generate_optimizers('AMSGrad','SGD',0.0001)\n",
        "AMSGrad_SGD_train_losses, AMSGrad_SGD_val_losses, AMSGrad_SGD_accuracy, AMSGrad_SGD_params = train(model, mw)\n",
        "model, mw = generate_optimizers('AMSGrad','Adam',0.0001)\n",
        "AMSGrad_Adam_train_losses, AMSGrad_Adam_val_losses, AMSGrad_Adam_accuracy, AMSGrad_Adam_params = train(model, mw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsQcDMJ9QDXf"
      },
      "source": [
        "\n",
        "### Test the optimizers with hyperoptimized hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SGD"
      ],
      "metadata": {
        "id": "oD2E5ZX3LVgg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxNAjpErQCxM"
      },
      "outputs": [],
      "source": [
        "# SGD_params = [(SGD_SGD_accuracy[-1], SGD_SGD_params),(SGD_Adam_accuracy[-1], SGD_Adam_params),(SGD_AdaGrad_accuracy[-1], SGD_AdaGrad_params),(SGD_RMSProp_accuracy[-1], SGD_RMSProp_params)]\n",
        "# params = sorted(SGD_params, key=lambda x: x[0])[-1][1]\n",
        "model = MNIST_FullyConnected(28 * 28, 128, 10).to(DEVICE)\n",
        "optimizer = SGD(0.1137143075466156)\n",
        "mw = ModuleWrapper(model, optimizer=optimizer)\n",
        "mw.initialize()\n",
        "Hyperoptimized_SGD_train_losses, Hyperoptimized_SGD_val_losses, Hyperoptimized_SGD_accuracy, _ = train(model, mw)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adam"
      ],
      "metadata": {
        "id": "dM167BxnLYH-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIFJ06eJy854"
      },
      "outputs": [],
      "source": [
        "Adam_params = [(Adam_SGD_accuracy[-1], Adam_SGD_params),(AdamBaydin_SGD_accuracy[-1], AdamBaydin_SGD_params),(Adam_Adam_accuracy[-1], Adam_Adam_params),(AdamBaydin_Adam_accuracy[-1], AdamBaydin_Adam_params)]\n",
        "params = sorted(Adam_params, key=lambda x: x[0])[-1][1]\n",
        "model = MNIST_FullyConnected(28 * 28, 128, 10).to(DEVICE)\n",
        "if 'beta1' in params:\n",
        "\n",
        "  optimizer = Adam(alpha=params['alpha'], beta1=params['beta1'],beta2=params['beta2'])\n",
        "else: \n",
        "  optimizer = Adam(alpha=params['alpha'])\n",
        "mw = ModuleWrapper(model, optimizer=optimizer)\n",
        "mw.initialize()\n",
        "Hyperoptimized_Adam_train_losses, Hyperoptimized_Adam_val_losses, Hyperoptimized_Adam_accuracy, _ = train(model, mw)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####AdaGrad"
      ],
      "metadata": {
        "id": "Di4D7tOYLfBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AdaGrad_params = [(AdaGrad_SGD_accuracy[-1], AdaGrad_AdaGrad_params),(AdaGrad_AdaGrad_accuracy[-1], AdaGrad_AdaGrad_params)]\n",
        "params = sorted(AdaGrad_params, key=lambda x: x[0])[-1][1]\n",
        "model = MNIST_FullyConnected(28 * 28, 128, 10).to(DEVICE)\n",
        "optimizer = AdaGrad(alpha=params['alpha'])\n",
        "mw = ModuleWrapper(model, optimizer=optimizer)\n",
        "mw.initialize()\n",
        "Hyperoptimized_AdaGrad_train_losses, Hyperoptimized_AdaGrad_val_losses, Hyperoptimized_AdaGrad_accuracy,_ = train(model, mw)"
      ],
      "metadata": {
        "id": "W2Pn-MBVLgab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RMSProp"
      ],
      "metadata": {
        "id": "Zt4l7ExLLk8Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B05KyG2e0pby"
      },
      "outputs": [],
      "source": [
        "optimizer = RMSProp(alpha= 0.0031602321243759945)\n",
        "mw = ModuleWrapper(model, optimizer=optimizer)\n",
        "mw.initialize()\n",
        "Hyperoptimized_RMSProp_train_losses, Hyperoptimized_RMSProp_val_losses, Hyperoptimized_RMSProp_accuracy,_ = train(model, mw)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AdaBelief"
      ],
      "metadata": {
        "id": "7iIiu5q-Lo7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AdaBelief\n",
        "optimizer = AdaBelief(alpha= 0.008482803590595722,beta1= 0.8999614715576172,beta2= 0.9989999532699585)\n",
        "mw = ModuleWrapper(model, optimizer=optimizer)\n",
        "mw.initialize()\n",
        "Hyperoptimized_AdaBelief_train_losses, Hyperoptimized_AdaBelief_val_losses, Hyperoptimized_AdaBelief_accuracy, _ = train(model, mw)"
      ],
      "metadata": {
        "id": "214qbQFyH6bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AMSGrad"
      ],
      "metadata": {
        "id": "rFPCXm7kLs_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AMSGrad(alpha= 0.001403641072101891,\n",
        "beta1= 0.8986871838569641,\n",
        "beta2= 0.9990024566650391)\n",
        "mw = ModuleWrapper(model, optimizer=optimizer)\n",
        "mw.initialize()\n",
        "Hyperoptimized_AMSGrad_train_losses, Hyperoptimized_AMSGrad_val_losses, Hyperoptimized_AMSGrad_accuracy, _ = train(model, mw)"
      ],
      "metadata": {
        "id": "9xWtR--sLv64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFfy9xKocU_M"
      },
      "source": [
        "### Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AKsY8uEwndD"
      },
      "source": [
        "#### SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frg2H5oIZOjY"
      },
      "outputs": [],
      "source": [
        "\n",
        "fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(8, 13))\n",
        "\n",
        "# Plot training loss on the top\n",
        "ax1.plot(SGD_train_losses, label='SGD')\n",
        "ax1.plot(SGD_SGD_train_losses, label='SGD with SGD hyperoptimizer')\n",
        "ax1.plot(SGD_Adam_train_losses, label='SGD with Adam hyperoptimizer')\n",
        "ax1.plot(SGD_AdaGrad_train_losses, label='SGD with AdaGrad hyperoptimizer')\n",
        "ax1.plot(SGD_RMSProp_train_losses, label='SGD wih RMSProp hyperoptimizer')\n",
        "\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "# Plot validation loss on the bottom\n",
        "ax2.plot(SGD_val_losses, label='SGD')\n",
        "ax2.plot(SGD_SGD_val_losses, label='SGD with SGD hyperoptimizer')\n",
        "ax2.plot(SGD_Adam_val_losses, label='SGD with Adam hyperoptimizer')\n",
        "ax2.plot(SGD_AdaGrad_val_losses, label='SGD with AdaGrad hyperoptimizer')\n",
        "ax2.plot(SGD_RMSProp_val_losses, label='SGD with RMSProp hyperoptimizer')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "\n",
        "ax3.plot(SGD_accuracy, label='SGD')\n",
        "ax3.plot(SGD_SGD_accuracy, label='SGD with SGD hyperoptimizer')\n",
        "ax3.plot(SGD_Adam_accuracy, label='SGD with Adam hyperoptimizer')\n",
        "ax3.plot(SGD_AdaGrad_accuracy, label='SGD with AdaGrad hyperoptimizer')\n",
        "ax3.plot(SGD_RMSProp_accuracy, label='SGD with RMSProp hyperoptimizer')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.set_title('Validation Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(8, 13))\n",
        "ax1.plot(SGD_train_losses, label='SGD')\n",
        "ax1.plot(Hyperoptimized_SGD_train_losses, label='SGD with tuned hyperparameters')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "ax2.plot(SGD_val_losses, label='SGD')\n",
        "ax2.plot(Hyperoptimized_SGD_val_losses, label='SGD with tuned hyperparameters')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "\n",
        "ax3.plot(SGD_accuracy, label='SGD')\n",
        "ax3.plot(Hyperoptimized_SGD_accuracy, label='SGD with tuned hyperparameters')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.set_title('Validation Accuracy')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adam"
      ],
      "metadata": {
        "id": "clvyO6zlJ6td"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBbjoQZgIzyX"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(8, 13))\n",
        "# Plot training loss on the top\n",
        "ax1.plot(Adam_train_losses, label='Adam')\n",
        "ax1.plot(Adam_SGD_train_losses, label='Adam with SGD hyperoptimizer')\n",
        "ax1.plot(AdamBaydin_SGD_train_losses, label='Adam with SGD hyperoptimizer (learning-rate only)')\n",
        "ax1.plot(Adam_Adam_train_losses, label='Adam with Adam hyperoptimizer')\n",
        "ax1.plot(AdamBaydin_Adam_train_losses, label='Adam with Adam hyperoptimizer (learning-rate only)')\n",
        "\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "# Plot validation loss on the bottom\n",
        "ax2.plot(Adam_val_losses, label='Adam')\n",
        "ax2.plot(Adam_SGD_val_losses, label='Adam with SGD hyperoptimizer')\n",
        "ax2.plot(AdamBaydin_SGD_val_losses, label='Adam with SGD hyperoptimizer (learning-rate only)')\n",
        "ax2.plot(Adam_Adam_val_losses, label='Adam with Adam hyperoptimizer')\n",
        "ax2.plot(AdamBaydin_Adam_val_losses, label='Adam with Adam hyperoptimizer (learning-rate only)')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "\n",
        "ax3.plot(Adam_accuracy, label='Adam')\n",
        "ax3.plot(Adam_SGD_accuracy, label='Adam with SGD hyperoptimizer')\n",
        "ax3.plot(AdamBaydin_SGD_accuracy, label='Adam with SGD hyperoptimizer (learning-rate only)')\n",
        "ax3.plot(Adam_Adam_accuracy, label='Adam with Adam hyperoptimizer')\n",
        "ax3.plot(AdamBaydin_Adam_accuracy, label='Adam with Adam hyperoptimizer (learning-rate only)')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.set_title('Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(8, 13))\n",
        "ax1.plot(Adam_train_losses, label='Adam')\n",
        "ax1.plot(Hyperoptimized_Adam_train_losses, label='Adam with tuned hyperparameters')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "ax2.plot(Adam_val_losses, label='Adam')\n",
        "ax2.plot(Hyperoptimized_Adam_val_losses, label='Adam with tuned hyperparameters')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "\n",
        "ax3.plot(Adam_accuracy, label='Adam')\n",
        "ax3.plot(Hyperoptimized_Adam_accuracy, label='Adam with tuned hyperparameters')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.set_title('Validation Accuracy')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AdaGrad"
      ],
      "metadata": {
        "id": "Fhj79mAcJ8hV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uR44bSiQJXF-"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(8, 13))\n",
        "# Plot training loss on the top\n",
        "ax1.plot(AdaGrad_train_losses, label='AdaGrad')\n",
        "ax1.plot(AdaGrad_SGD_train_losses, label='AdaGrad with SGD hyperoptimizer')\n",
        "ax1.plot(AdaGrad_AdaGrad_train_losses, label='AdaGrad with AdaGrad hyperoptimizer')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "# Plot validation loss on the bottom\n",
        "ax2.plot(AdaGrad_val_losses, label='AdaGrad')\n",
        "ax2.plot(AdaGrad_SGD_val_losses, label='AdaGrad with SGD hyperoptimizer')\n",
        "ax2.plot(AdaGrad_AdaGrad_val_losses, label='AdaGrad with AdaGrad hyperoptimizer')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "\n",
        "\n",
        "ax3.plot(AdaGrad_accuracy, label='AdaGrad')\n",
        "ax3.plot(AdaGrad_SGD_accuracy, label='AdaGrad with SGD hyperoptimizer')\n",
        "ax3.plot(AdaGrad_AdaGrad_accuracy, label='AdaGrad with AdaGrad hyperoptimizer')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.set_title('Validation Loss')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(8, 13))\n",
        "ax1.plot(AdaGrad_train_losses, label='AdaGrad')\n",
        "ax1.plot(Hyperoptimized_AdaGrad_train_losses, label='AdaGrad with tuned hyperparameters')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "ax2.plot(AdaGrad_val_losses, label='AdaGrad')\n",
        "ax2.plot(Hyperoptimized_AdaGrad_val_losses, label='AdaGrad with tuned hyperparameters')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "\n",
        "ax3.plot(AdaGrad_accuracy, label='AdaGrad')\n",
        "ax3.plot(Hyperoptimized_AdaGrad_accuracy, label='AdaGrad with tuned hyperparameters')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.set_title('Validation Accuracy')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RMSProp"
      ],
      "metadata": {
        "id": "133ksdRKKfiO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WppBrLhKdWBt"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2,ax3) = plt.subplots(3,1, figsize=(8, 13))\n",
        "# Plot training loss on the top\n",
        "ax1.plot(RMSProp_train_losses, label='RMSProp')\n",
        "ax1.plot(RMSPropAlpha_SGD_train_losses, label='RMSProp with SGD hyperoptimizer (learning rate only)')\n",
        "ax1.plot(RMSProp_SGD_train_losses, label='RMSProp with SGD hyperoptimizer')\n",
        "ax1.plot(RMSPropAlpha_RMSProp_train_losses, label='RMSProp with RMSProp hyperoptimizer (learning rate only)')\n",
        "ax1.plot(RMSProp_RMSProp_train_losses, label='RMSProp with RMSProp hyperoptimizer')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "# Plot validation loss on the bottom\n",
        "ax2.plot(RMSProp_val_losses, label='RMSProp')\n",
        "ax2.plot(RMSPropAlpha_SGD_val_losses, label='RMSProp with SGD hyperoptimizer (learning rate only)')\n",
        "ax2.plot(RMSProp_SGD_val_losses, label='RMSProp with SGD hyperoptimizer')\n",
        "ax2.plot(RMSPropAlpha_RMSProp_val_losses, label='RMSProp with RMSProp hyperoptimizer (learning rate only)')\n",
        "ax2.plot(RMSProp_RMSProp_val_losses, label='RMSProp with RMSProp hyperoptimizer')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "\n",
        "ax3.plot(RMSProp_accuracy, label='RMSProp')\n",
        "ax3.plot(RMSPropAlpha_SGD_accuracy, label='RMSProp with SGD hyperoptimizer (learning rate only)')\n",
        "ax3.plot(RMSProp_SGD_accuracy, label='RMSProp with SGD hyperoptimizer')\n",
        "ax3.plot(RMSPropAlpha_RMSProp_accuracy, label='RMSProp with RMSProp hyperoptimizer (learning rate only)')\n",
        "ax3.plot(RMSProp_RMSProp_accuracy, label='RMSProp with RMSProp hyperoptimizer')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Loss')\n",
        "ax3.legend()\n",
        "ax3.set_title('Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(8, 13))\n",
        "ax1.plot(RMSProp_train_losses, label='RMSProp')\n",
        "ax1.plot(Hyperoptimized_RMSProp_train_losses, label='RMSProp with tuned hyperparameters')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "ax2.plot(RMSProp_val_losses, label='RMSProp')\n",
        "ax2.plot(Hyperoptimized_RMSProp_val_losses, label='RMSProp with tuned hyperparameters')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "\n",
        "ax3.plot(RMSProp_accuracy, label='RMSProp')\n",
        "ax3.plot(Hyperoptimized_RMSProp_accuracy, label='RMSProp with tuned hyperparameters')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.set_title('Validation Accuracy')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AdaBelief"
      ],
      "metadata": {
        "id": "cdSsPzAOKh2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2,ax3) = plt.subplots(3,1, figsize=(8, 13))\n",
        "# Plot training loss on the top\n",
        "ax1.plot(AdaBelief_train_losses, label='AdaBelief')\n",
        "ax1.plot(AdaBelief_SGD_train_losses, label='AdaBelief with SGD hyperoptimizer')\n",
        "ax1.plot(AdaBelief_Adam_train_losses, label='AdaBelief with Adam hyperoptimizer')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "# Plot validation loss on the bottom\n",
        "ax2.plot(AdaBelief_val_losses, label='AdaBelief')\n",
        "ax2.plot(AdaBelief_SGD_val_losses, label='AdaBelief with SGD hyperoptimizer')\n",
        "ax2.plot(AdaBelief_Adam_val_losses, label='AdaBelief with Adam hyperoptimizer')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "\n",
        "ax3.plot(AdaBelief_accuracy, label='AdaBelief')\n",
        "ax3.plot(AdaBelief_SGD_accuracy, label='AdaBelief with SGD hyperoptimizer')\n",
        "ax3.plot(AdaBelief_Adam_accuracy, label='AdaBelief with Adam hyperoptimizer')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Loss')\n",
        "ax3.legend()\n",
        "ax3.set_title('Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(8, 13))\n",
        "ax1.plot(AdaBelief_train_losses, label='AdaBelief')\n",
        "ax1.plot(Hyperoptimized_AdaBelief_train_losses, label='AdaBelief with tuned hyperparameters')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "ax2.plot(AdaBelief_val_losses, label='AdaBelief')\n",
        "ax2.plot(Hyperoptimized_AdaBelief_val_losses, label='AdaBelief with tuned hyperparameters')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "\n",
        "ax3.plot(AdaBelief_accuracy, label='AdaBelief')\n",
        "ax3.plot(Hyperoptimized_AdaBelief_accuracy, label='AdaBelief with tuned hyperparameters')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.set_title('Validation Accuracy')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6KseDwFRKpq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AMSGrad"
      ],
      "metadata": {
        "id": "kAvB66FdKjig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2,ax3) = plt.subplots(3,1, figsize=(8, 13))\n",
        "# Plot training loss on the top\n",
        "ax1.plot(AMSGrad_train_losses, label='AMSGrad')\n",
        "ax1.plot(AMSGrad_SGD_train_losses, label='AMSGrad with SGD hyperoptimizer')\n",
        "ax1.plot(AMSGrad_Adam_train_losses, label='AMSGrad with Adam hyperoptimizer')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "# Plot validation loss on the bottom\n",
        "ax2.plot(AMSGrad_val_losses, label='AMSGrad')\n",
        "ax2.plot(AMSGrad_SGD_val_losses, label='AMSGrad with SGD hyperoptimizer')\n",
        "ax2.plot(AMSGrad_Adam_val_losses, label='AMSGrad with Adam hyperoptimizer')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "\n",
        "ax3.plot(AMSGrad_accuracy, label='AMSGrad')\n",
        "ax3.plot(AMSGrad_SGD_accuracy, label='AMSGrad with SGD hyperoptimizer')\n",
        "ax3.plot(AMSGrad_Adam_accuracy, label='AMSGrad with Adam hyperoptimizer')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Loss')\n",
        "ax3.legend()\n",
        "ax3.set_title('Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(8, 13))\n",
        "ax1.plot(AMSGrad_train_losses, label='AMSGrad')\n",
        "ax1.plot(Hyperoptimized_AMSGrad_train_losses, label='AMSGrad with tuned hyperparameters')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "ax2.plot(AMSGrad_val_losses, label='AMSGrad')\n",
        "ax2.plot(Hyperoptimized_AMSGrad_val_losses, label='AMSGrad with tuned hyperparameters')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "\n",
        "ax3.plot(AMSGrad_accuracy, label='AMSGrad')\n",
        "ax3.plot(Hyperoptimized_AMSGrad_accuracy, label='AMSGrad with tuned hyperparameters')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.set_title('Validation Accuracy')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KZWH3HIWLjVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-hM0wdaqu89"
      },
      "source": [
        "## Part 2: Extension of the original paper.\n",
        "In this part, we will construct a more sophisticated neural network, UNet, and apply the concept of hyperoptimization to implement hyperoptimized AdaBelief and AMSGrad. The performance of the hyperoptimzers and classical optimizers on UNet will be compared. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CIFAR-10 Setup Code from CSC413 Assignment 2"
      ],
      "metadata": {
        "id": "5MJd2HjHNP_w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja2meGKI5X88"
      },
      "source": [
        "#### import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piDmAsqFG0gU"
      },
      "outputs": [],
      "source": [
        "######################################################################\n",
        "# Setup working directory\n",
        "######################################################################\n",
        "%mkdir -p /content/csc413/a2/\n",
        "%cd /content/csc413/a2\n",
        "\n",
        "######################################################################\n",
        "# Helper functions for loading data\n",
        "######################################################################\n",
        "# adapted from\n",
        "# https://github.com/fchollet/keras/blob/master/keras/datasets/cifar10.py\n",
        "HORSE_CATEGORY=7\n",
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "import tarfile\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "\n",
        "\n",
        "def get_file(fname, origin, untar=False, extract=False, archive_format=\"auto\", cache_dir=\"data\"):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + \".tar.gz\"\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "\n",
        "    print(\"File path: %s\" % fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print(\"Downloading data from\", origin)\n",
        "\n",
        "        error_msg = \"URL fetch failure on {}: {} -- {}\"\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print(\"Extracting file.\")\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "\n",
        "def load_batch(fpath, label_key=\"labels\"):\n",
        "    \"\"\"Internal utility for parsing CIFAR data.\n",
        "    # Arguments\n",
        "        fpath: path the file to parse.\n",
        "        label_key: key for label data in the retrieve\n",
        "            dictionary.\n",
        "    # Returns\n",
        "        A tuple `(data, labels)`.\n",
        "    \"\"\"\n",
        "    f = open(fpath, \"rb\")\n",
        "    if sys.version_info < (3,):\n",
        "        d = pickle.load(f)\n",
        "    else:\n",
        "        d = pickle.load(f, encoding=\"bytes\")\n",
        "        # decode utf8\n",
        "        d_decoded = {}\n",
        "        for k, v in d.items():\n",
        "            d_decoded[k.decode(\"utf8\")] = v\n",
        "        d = d_decoded\n",
        "    f.close()\n",
        "    data = d[\"data\"]\n",
        "    labels = d[label_key]\n",
        "\n",
        "    data = data.reshape(data.shape[0], 3, 32, 32)\n",
        "    return data, labels\n",
        "\n",
        "\n",
        "def load_cifar10(transpose=False):\n",
        "    \"\"\"Loads CIFAR10 dataset.\n",
        "    # Returns\n",
        "        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
        "    \"\"\"\n",
        "    dirname = \"cifar-10-batches-py\"\n",
        "    origin = \"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "    path = get_file(dirname, origin=origin, untar=True)\n",
        "\n",
        "    num_train_samples = 50000\n",
        "\n",
        "    x_train = np.zeros((num_train_samples, 3, 32, 32), dtype=\"uint8\")\n",
        "    y_train = np.zeros((num_train_samples,), dtype=\"uint8\")\n",
        "\n",
        "    for i in range(1, 6):\n",
        "        fpath = os.path.join(path, \"data_batch_\" + str(i))\n",
        "        data, labels = load_batch(fpath)\n",
        "        x_train[(i - 1) * 10000 : i * 10000, :, :, :] = data\n",
        "        y_train[(i - 1) * 10000 : i * 10000] = labels\n",
        "\n",
        "    fpath = os.path.join(path, \"test_batch\")\n",
        "    x_test, y_test = load_batch(fpath)\n",
        "\n",
        "    y_train = np.reshape(y_train, (len(y_train), 1))\n",
        "    y_test = np.reshape(y_test, (len(y_test), 1))\n",
        "\n",
        "    if transpose:\n",
        "        x_train = x_train.transpose(0, 2, 3, 1)\n",
        "        x_test = x_test.transpose(0, 2, 3, 1)\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jIKvJNtoVgU"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7fti3cryStt"
      },
      "outputs": [],
      "source": [
        "# Download cluster centers for k-means over colours\n",
        "colours_fpath = get_file(\n",
        "    fname=\"colours\", origin=\"http://www.cs.toronto.edu/~jba/kmeans_colour_a2.tar.gz\", untar=True\n",
        ")\n",
        "# Download CIFAR dataset\n",
        "m = load_cifar10()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN3FF8hIlS7h"
      },
      "source": [
        "#### Data related code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-900ROTMlSPd"
      },
      "outputs": [],
      "source": [
        "def get_rgb_cat(xs, colours):\n",
        "    \"\"\"\n",
        "    Get colour categories given RGB values. This function doesn't\n",
        "    actually do the work, instead it splits the work into smaller\n",
        "    chunks that can fit into memory, and calls helper function\n",
        "    _get_rgb_cat\n",
        "\n",
        "    Args:\n",
        "      xs: float numpy array of RGB images in [B, C, H, W] format\n",
        "      colours: numpy array of colour categories and their RGB values\n",
        "    Returns:\n",
        "      result: int numpy array of shape [B, 1, H, W]\n",
        "    \"\"\"\n",
        "    if np.shape(xs)[0] < 100:\n",
        "        return _get_rgb_cat(xs)\n",
        "    batch_size = 100\n",
        "    nexts = []\n",
        "    for i in range(0, np.shape(xs)[0], batch_size):\n",
        "        next = _get_rgb_cat(xs[i : i + batch_size, :, :, :], colours)\n",
        "        nexts.append(next)\n",
        "    result = np.concatenate(nexts, axis=0)\n",
        "    return result\n",
        "\n",
        "\n",
        "def _get_rgb_cat(xs, colours):\n",
        "    \"\"\"\n",
        "    Get colour categories given RGB values. This is done by choosing\n",
        "    the colour in `colours` that is the closest (in RGB space) to\n",
        "    each point in the image `xs`. This function is a little memory\n",
        "    intensive, and so the size of `xs` should not be too large.\n",
        "\n",
        "    Args:\n",
        "      xs: float numpy array of RGB images in [B, C, H, W] format\n",
        "      colours: numpy array of colour categories and their RGB values\n",
        "    Returns:\n",
        "      result: int numpy array of shape [B, 1, H, W]\n",
        "    \"\"\"\n",
        "    num_colours = np.shape(colours)[0]\n",
        "    xs = np.expand_dims(xs, 0)\n",
        "    cs = np.reshape(colours, [num_colours, 1, 3, 1, 1])\n",
        "    dists = np.linalg.norm(xs - cs, axis=2)  # 2 = colour axis\n",
        "    cat = np.argmin(dists, axis=0)\n",
        "    cat = np.expand_dims(cat, axis=1)\n",
        "    return cat\n",
        "\n",
        "\n",
        "def get_cat_rgb(cats, colours):\n",
        "    \"\"\"\n",
        "    Get RGB colours given the colour categories\n",
        "\n",
        "    Args:\n",
        "      cats: integer numpy array of colour categories\n",
        "      colours: numpy array of colour categories and their RGB values\n",
        "    Returns:\n",
        "      numpy tensor of RGB colours\n",
        "    \"\"\"\n",
        "    return colours[cats]\n",
        "\n",
        "\n",
        "def process(xs, ys, max_pixel=256.0, downsize_input=False):\n",
        "    \"\"\"\n",
        "    Pre-process CIFAR10 images by taking only the horse category,\n",
        "    shuffling, and have colour values be bound between 0 and 1\n",
        "\n",
        "    Args:\n",
        "      xs: the colour RGB pixel values\n",
        "      ys: the category labels\n",
        "      max_pixel: maximum pixel value in the original data\n",
        "    Returns:\n",
        "      xs: value normalized and shuffled colour images\n",
        "      grey: greyscale images, also normalized so values are between 0 and 1\n",
        "    \"\"\"\n",
        "    xs = xs / max_pixel\n",
        "    xs = xs[np.where(ys == HORSE_CATEGORY)[0], :, :, :]\n",
        "    npr.shuffle(xs)\n",
        "\n",
        "    grey = np.mean(xs, axis=1, keepdims=True)\n",
        "\n",
        "    if downsize_input:\n",
        "        downsize_module = nn.Sequential(\n",
        "            nn.AvgPool2d(2),\n",
        "            nn.AvgPool2d(2),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "        )\n",
        "        xs_downsized = downsize_module.forward(torch.from_numpy(xs).float())\n",
        "        xs_downsized = xs_downsized.data.numpy()\n",
        "        return (xs, xs_downsized)\n",
        "    else:\n",
        "        return (xs, grey)\n",
        "\n",
        "\n",
        "def get_batch(x, y, batch_size):\n",
        "    \"\"\"\n",
        "    Generated that yields batches of data\n",
        "\n",
        "    Args:\n",
        "      x: input values\n",
        "      y: output values\n",
        "      batch_size: size of each batch\n",
        "    Yields:\n",
        "      batch_x: a batch of inputs of size at most batch_size\n",
        "      batch_y: a batch of outputs of size at most batch_size\n",
        "    \"\"\"\n",
        "    N = np.shape(x)[0]\n",
        "    assert N == np.shape(y)[0]\n",
        "    for i in range(0, N, batch_size):\n",
        "        batch_x = x[i : i + batch_size, :, :, :]\n",
        "        batch_y = y[i : i + batch_size, :, :, :]\n",
        "        yield (batch_x, batch_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft0qRJgWlK2q"
      },
      "source": [
        "#### Torch helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgMknlyhlJvi"
      },
      "outputs": [],
      "source": [
        "def get_torch_vars(xs, ys, gpu=False):\n",
        "    \"\"\"\n",
        "    Helper function to convert numpy arrays to pytorch tensors.\n",
        "    If GPU is used, move the tensors to GPU.\n",
        "\n",
        "    Args:\n",
        "      xs (float numpy tenosor): greyscale input\n",
        "      ys (int numpy tenosor): categorical labels\n",
        "      gpu (bool): whether to move pytorch tensor to GPU\n",
        "    Returns:\n",
        "      Variable(xs), Variable(ys)\n",
        "    \"\"\"\n",
        "    xs = torch.from_numpy(xs).float()\n",
        "    ys = torch.from_numpy(ys).long()\n",
        "    if gpu:\n",
        "        xs = xs.cuda()\n",
        "        ys = ys.cuda()\n",
        "    return Variable(xs), Variable(ys)\n",
        "\n",
        "\n",
        "def compute_loss(criterion, outputs, labels, batch_size, num_colours):\n",
        "    \"\"\"\n",
        "    Helper function to compute the loss. Since this is a pixelwise\n",
        "    prediction task we need to reshape the output and ground truth\n",
        "    tensors into a 2D tensor before passing it in to the loss criteron.\n",
        "\n",
        "    Args:\n",
        "      criterion: pytorch loss criterion\n",
        "      outputs (pytorch tensor): predicted labels from the model\n",
        "      labels (pytorch tensor): ground truth labels\n",
        "      batch_size (int): batch size used for training\n",
        "      num_colours (int): number of colour categories\n",
        "    Returns:\n",
        "      pytorch tensor for loss\n",
        "    \"\"\"\n",
        "    batch = outputs.size(0)\n",
        "    loss_out = outputs.transpose(1, 3).contiguous().view([batch * 32 * 32, num_colours])\n",
        "    loss_lab = labels.transpose(1, 3).contiguous().view([batch * 32 * 32])\n",
        "    return criterion(loss_out, loss_lab)\n",
        "\n",
        "\n",
        "def run_validation_step(\n",
        "    cnn,\n",
        "    criterion,\n",
        "    test_grey,\n",
        "    test_rgb_cat,\n",
        "    batch_size,\n",
        "    colours,\n",
        "    plotpath=None,\n",
        "    visualize=True,\n",
        "    downsize_input=False\n",
        "):\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "    losses = []\n",
        "    num_colours = np.shape(colours)[0]\n",
        "    for i, (xs, ys) in enumerate(get_batch(test_grey, test_rgb_cat, batch_size)):\n",
        "        images, labels = get_torch_vars(xs, ys, args.gpu)\n",
        "        outputs = cnn(images)\n",
        "\n",
        "        val_loss = compute_loss(\n",
        "            criterion, outputs, labels, batch_size=args.batch_size, num_colours=num_colours\n",
        "        )\n",
        "        losses.append(val_loss.data.item())\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1, keepdim=True)\n",
        "        total += labels.size(0) * 32 * 32\n",
        "        correct += (predicted == labels.data).sum()\n",
        "\n",
        "    if plotpath:  # only plot if a path is provided\n",
        "        plot(\n",
        "            xs,\n",
        "            ys,\n",
        "            predicted.cpu().numpy(),\n",
        "            colours,\n",
        "            plotpath,\n",
        "            visualize=visualize,\n",
        "            compare_bilinear=downsize_input,\n",
        "        )\n",
        "\n",
        "    val_loss = np.mean(losses)\n",
        "    val_acc = 100 * correct / total\n",
        "    return val_loss, val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp_wCpMjqt5w"
      },
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syg8NjwMqw_F"
      },
      "outputs": [],
      "source": [
        "def plot(input, gtlabel, output, colours, path, visualize=True, compare_bilinear=False):\n",
        "    \"\"\"\n",
        "    Generate png plots of input, ground truth, and outputs\n",
        "\n",
        "    Args:\n",
        "      input: the greyscale input to the colourization CNN\n",
        "      gtlabel: the grouth truth categories for each pixel\n",
        "      output: the predicted categories for each pixel\n",
        "      colours: numpy array of colour categories and their RGB values\n",
        "      path: output path\n",
        "      visualize: display the figures inline or save the figures in path\n",
        "    \"\"\"\n",
        "    grey = np.transpose(input[:10, :, :, :], [0, 2, 3, 1])\n",
        "    gtcolor = get_cat_rgb(gtlabel[:10, 0, :, :], colours)\n",
        "    predcolor = get_cat_rgb(output[:10, 0, :, :], colours)\n",
        "\n",
        "    img_stack = [np.hstack(np.tile(grey, [1, 1, 1, 3])), np.hstack(gtcolor), np.hstack(predcolor)]\n",
        "\n",
        "    if compare_bilinear:\n",
        "        downsize_module = nn.Sequential(\n",
        "            nn.AvgPool2d(2),\n",
        "            nn.AvgPool2d(2),\n",
        "            nn.Upsample(scale_factor=2, mode=\"bilinear\"),\n",
        "            nn.Upsample(scale_factor=2, mode=\"bilinear\"),\n",
        "        )\n",
        "        gt_input = np.transpose(\n",
        "            gtcolor,\n",
        "            [\n",
        "                0,\n",
        "                3,\n",
        "                1,\n",
        "                2\n",
        "            ],\n",
        "        )\n",
        "        color_bilinear = downsize_module.forward(torch.from_numpy(gt_input).float())\n",
        "        color_bilinear = np.transpose(color_bilinear.data.numpy(), [0, 2, 3, 1])\n",
        "        img_stack = [\n",
        "            np.hstack(np.transpose(input[:10, :, :, :], [0, 2, 3, 1])),\n",
        "            np.hstack(gtcolor),\n",
        "            np.hstack(predcolor),\n",
        "            np.hstack(color_bilinear),\n",
        "        ]\n",
        "    img = np.vstack(img_stack)\n",
        "\n",
        "    plt.grid(None)\n",
        "    plt.imshow(img, vmin=0.0, vmax=1.0)\n",
        "    plt.show()\n",
        "    # else:\n",
        "    #     plt.savefig(path)\n",
        "\n",
        "\n",
        "def toimage(img, cmin, cmax):\n",
        "    return Image.fromarray((img.clip(cmin, cmax) * 255).astype(np.uint8))\n",
        "\n",
        "\n",
        "def plot_activation(args, cnn):\n",
        "    # LOAD THE COLOURS CATEGORIES\n",
        "    colours = np.load(args.colours, allow_pickle=True)[0]\n",
        "    num_colours = np.shape(colours)[0]\n",
        "\n",
        "    (x_train, y_train), (x_test, y_test) = load_cifar10()\n",
        "    test_rgb, test_grey = process(x_test, y_test, downsize_input=args.downsize_input)\n",
        "    test_rgb_cat = get_rgb_cat(test_rgb, colours)\n",
        "\n",
        "    # Take the idnex of the test image\n",
        "    id = args.index\n",
        "    # outdir = \"outputs/\" + args.experiment_name + \"/act\" + str(id)\n",
        "    # if not os.path.exists(outdir):\n",
        "    #     os.makedirs(outdir)\n",
        "    images, labels = get_torch_vars(\n",
        "        np.expand_dims(test_grey[id], 0), np.expand_dims(test_rgb_cat[id], 0)\n",
        "    )\n",
        "    cnn.cpu()\n",
        "    outputs = cnn(images)\n",
        "    _, predicted = torch.max(outputs.data, 1, keepdim=True)\n",
        "    predcolor = get_cat_rgb(predicted.cpu().numpy()[0, 0, :, :], colours)\n",
        "    img = predcolor\n",
        "    # toimage(predcolor, cmin=0, cmax=1).save(os.path.join(outdir, \"output_%d.png\" % id))\n",
        "\n",
        "    if not args.downsize_input:\n",
        "        img = np.tile(np.transpose(test_grey[id], [1, 2, 0]), [1, 1, 3])\n",
        "    else:\n",
        "        img = np.transpose(test_grey[id], [1, 2, 0])\n",
        "    # toimage(img, cmin=0, cmax=1).save(os.path.join(outdir, \"input_%d.png\" % id))\n",
        "\n",
        "    img = np.transpose(test_rgb[id], [1, 2, 0])\n",
        "    # toimage(img, cmin=0, cmax=1).save(os.path.join(outdir, \"input_%d_gt.png\" % id))\n",
        "\n",
        "    def add_border(img):\n",
        "        return np.pad(img, 1, \"constant\", constant_values=1.0)\n",
        "\n",
        "    def draw_activations(path, activation, imgwidth=4):\n",
        "        img = np.vstack(\n",
        "            [\n",
        "                np.hstack(\n",
        "                    [\n",
        "                        add_border(filter)\n",
        "                        for filter in activation[i * imgwidth : (i + 1) * imgwidth, :, :]\n",
        "                    ]\n",
        "                )\n",
        "                for i in range(activation.shape[0] // imgwidth)\n",
        "            ]\n",
        "        )\n",
        "        scipy.misc.imsave(path, img)\n",
        "\n",
        "    # for i, tensor in enumerate([cnn.out1, cnn.out2, cnn.out3, cnn.out4, cnn.out5]):\n",
        "    #     draw_activations(\n",
        "    #         os.path.join(outdir, \"conv%d_out_%d.png\" % (i + 1, id)), tensor.data.cpu().numpy()[0]\n",
        "    #     )\n",
        "    # print(\"visualization results are saved to %s\" % outdir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UNet & Training Function (From Assignment 2)"
      ],
      "metadata": {
        "id": "DQ6Fdd2BNcOj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb7Agthjf9qs"
      },
      "source": [
        "#### UNet "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fo0lBy8GDcbc"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, kernel, num_filters, num_colours, num_in_channels):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Useful parameters\n",
        "        stride = 2\n",
        "        padding = kernel // 2\n",
        "        output_padding = 1\n",
        "        self.first = nn.Sequential(\n",
        "            nn.Conv2d(num_in_channels, num_filters, kernel, stride=2, padding = kernel // 2),\n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.second = nn.Sequential(\n",
        "            nn.Conv2d(num_filters, 2*num_filters, kernel, stride=2, padding = kernel // 2),\n",
        "            nn.BatchNorm2d(2*num_filters),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.third = nn.Sequential(\n",
        "            nn.ConvTranspose2d(2*num_filters,num_filters, kernel_size=kernel,stride=2,dilation = 1,padding=padding,output_padding=1),\n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fourth = nn.Sequential(\n",
        "            nn.ConvTranspose2d(2*num_filters,num_colours, kernel_size=kernel,stride=2,dilation = 1,padding=padding,output_padding=1),\n",
        "            nn.BatchNorm2d(num_colours),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fifth = nn.Conv2d(num_colours+num_in_channels, num_colours, kernel,padding = kernel // 2)\n",
        "\n",
        "    def initialize(self):\n",
        "        nn.init.kaiming_uniform_(self.first[0].weight, a=math.sqrt(5))\n",
        "        nn.init.kaiming_uniform_(self.second[0].weight, a=math.sqrt(5))\n",
        "        nn.init.kaiming_uniform_(self.third[0].weight, a=math.sqrt(5))\n",
        "        nn.init.kaiming_uniform_(self.fourth[0].weight, a=math.sqrt(5))\n",
        "        nn.init.kaiming_uniform_(self.fifth.weight, a=math.sqrt(5))\n",
        "\n",
        "    def forward(self, x):\n",
        "        first = self.first(x)\n",
        "        second = self.second(first)\n",
        "        third = self.third(second)\n",
        "\n",
        "        fourth = self.fourth(torch.cat([first, third], dim=1))\n",
        "\n",
        "        output = self.fifth(torch.cat([x, fourth], dim=1))\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBAS9YQCfkq1"
      },
      "source": [
        "#### Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfNyqj7lfkq1"
      },
      "outputs": [],
      "source": [
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "def train_UNet(args, optimizer,cnn=None):\n",
        "    opt = optimizer\n",
        "    torch.set_num_threads(5)\n",
        "    # Numpy random seed\n",
        "    npr.seed(args.seed)\n",
        "    save_dir = \"outputs/\" + args.experiment_name\n",
        "    # LOAD THE COLOURS CATEGORIES\n",
        "    colours = np.load(args.colours, allow_pickle=True, encoding=\"bytes\")[0]\n",
        "    num_colours = np.shape(colours)[0]\n",
        "    # INPUT CHANNEL\n",
        "    num_in_channels = 1 if not args.downsize_input else 3\n",
        "    # LOAD THE MODEL\n",
        "    if cnn is None:\n",
        "        Net = globals()[args.model]\n",
        "        cnn = Net(args.kernel, args.num_filters, num_colours, num_in_channels)\n",
        "    cnn.initialize()\n",
        "    # LOSS FUNCTION\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # DATA\n",
        "    print(\"Loading data...\")\n",
        "    (x_train, y_train), (x_test, y_test) = load_cifar10()\n",
        "\n",
        "    print(\"Transforming data...\")\n",
        "    train_rgb, train_grey = process(x_train, y_train, downsize_input=args.downsize_input)\n",
        "    train_rgb_cat = get_rgb_cat(train_rgb, colours)\n",
        "    test_rgb, test_grey = process(x_test, y_test, downsize_input=args.downsize_input)\n",
        "    test_rgb_cat = get_rgb_cat(test_rgb, colours)\n",
        "\n",
        "    print(\"Beginning training ...\")\n",
        "    if args.gpu:\n",
        "        cnn.cuda()\n",
        "    \n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    valid_accs = []\n",
        "    total_time = 0\n",
        "    \n",
        "    mw = ModuleWrapper(cnn, optimizer=opt)\n",
        "    mw.initialize()\n",
        "    for epoch in range(args.epochs):\n",
        "        # Train the Model\n",
        "        start_time = time.time()\n",
        "        cnn.train()  # Change model to 'train' mode\n",
        "        \n",
        "        losses = []\n",
        "        for i, (xs, ys) in enumerate(get_batch(train_grey, train_rgb_cat, args.batch_size)):\n",
        "            mw.begin()\n",
        "            images, labels = get_torch_vars(xs, ys, args.gpu)\n",
        "            pred = mw.forward(images)\n",
        "            loss = compute_loss(\n",
        "                criterion, pred, labels, batch_size=args.batch_size, num_colours=num_colours\n",
        "            )\n",
        "            # Forward + Backward + Optimize\n",
        "            mw.zero_grad()\n",
        "\n",
        "            \n",
        "            loss.backward(create_graph=True)\n",
        "            mw.step()\n",
        "            losses.append(loss.data.item())\n",
        "        time_elapsed = time.time() - start_time\n",
        "        avg_loss = np.mean(losses)\n",
        "        train_losses.append(avg_loss)\n",
        "        \n",
        "        print(\n",
        "            \"Epoch [%d/%d], Loss: %.4f, Time (s): %.2f\"\n",
        "            % (epoch + 1, args.epochs, avg_loss, time_elapsed)\n",
        "        )\n",
        "        total_time += time_elapsed\n",
        "\n",
        "        # Evaluate the model\n",
        "        cnn.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
        "        val_loss, val_acc = run_validation_step(\n",
        "            cnn,\n",
        "            criterion,\n",
        "            test_grey,\n",
        "            test_rgb_cat,\n",
        "            args.batch_size,\n",
        "            colours,\n",
        "            save_dir + \"/test_%d.png\" % epoch,\n",
        "            args.visualize,\n",
        "            args.downsize_input,\n",
        "        )\n",
        "\n",
        "        valid_losses.append(val_loss)\n",
        "        valid_accs.append(val_acc.item())\n",
        "        print(\n",
        "            \"Epoch [%d/%d], Val Loss: %.4f, Val Acc: %.1f%%\"\n",
        "            % (epoch + 1, args.epochs, val_loss, val_acc)\n",
        "        )\n",
        "        final_parameters={}\n",
        "        for i in mw.optimizer.parameters:\n",
        "          if ((type(mw.optimizer)==Adam or type(mw.optimizer)==AdamBaydin or type(mw.optimizer)==AMSGrad or type(mw.optimizer)==AdaBelief) \n",
        "          and (i in {'beta1','beta2'})) or ((type(mw.optimizer)==RMSProp or type(mw.optimizer)==RMSPropAlpha) and i in {'gamma'}):\n",
        "            final_parameters[i] = Adam.clamp(mw.optimizer.parameters[i]).item()\n",
        "          elif (type(mw.optimizer)==RMSProp or type(mw.optimizer)==RMSPropAlpha) and i=='Alpha':\n",
        "            final_parameters[i] = mw.optimizer.parameters[i].item()**2\n",
        "          else:\n",
        "            final_parameters[i] = mw.optimizer.parameters[i].item()\n",
        "        print(final_parameters)\n",
        "    print('Total training time: {:.2f} seconds'.format(total_time))\n",
        "\n",
        "\n",
        "    return cnn, train_losses, valid_losses, valid_accs, final_parameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yVde2xPXPFGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FuGPIgg4LW6"
      },
      "source": [
        "#### Argument Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-vGs7qHndmY"
      },
      "outputs": [],
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"\",\n",
        "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
        "    \"model\": \"UNet\",\n",
        "    \"kernel\": 3,\n",
        "    \"num_filters\": 32,\n",
        "    'learn_rate':0.001, \n",
        "    \"batch_size\": 100,\n",
        "    \"epochs\": 50,\n",
        "    \"seed\": 0,\n",
        "    \"plot\": True,\n",
        "    \"experiment_name\": \"colourization_cnn\",\n",
        "    \"visualize\": True,\n",
        "    \"downsize_input\": False,\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m9yV45ImGSE"
      },
      "source": [
        "### Train by classical and hyperoptimized optimizers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SGD"
      ],
      "metadata": {
        "id": "4M-U5QqaHrSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = train_UNet(args, optimizer=AdaBelief())"
      ],
      "metadata": {
        "id": "7rk8ecL0UBhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = train_UNet(args, optimizer=AdaBeliefBaydin())"
      ],
      "metadata": {
        "id": "1BLoKdDAUG_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn, SGD_train_losses, SGD_valid_losses, SGD_valid_accs, SGD_params = train_UNet(args, optimizer=SGD())"
      ],
      "metadata": {
        "id": "gPQjYI0-HsYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn, SGD_SGD_train_losses, SGD_SGD_valid_losses, SGD_SGD_valid_accs, SGD_params = train_UNet(args, optimizer=SGD(optimizer=SGD()))"
      ],
      "metadata": {
        "id": "Rbyunk7lHt5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn, SGD_Adam_train_losses, SGD_Adam_valid_losses, SGD_Adam_valid_accs, SGD_Adam_params = train_UNet(args, optimizer=SGD(optimizer=Adam(1e-5)))"
      ],
      "metadata": {
        "id": "1CTeigNzHudv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn, SGD_AMSGrad_train_losses, SGD_AMSGrad_valid_losses, SGD_AMSGrad_valid_accs, SGD_AMSGrad_params = train_UNet(args, optimizer=SGD(optimizer=AMSGrad(1e-5)))"
      ],
      "metadata": {
        "id": "jbCJvMBpOMFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adam"
      ],
      "metadata": {
        "id": "EiEcaPEmEwiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn, Adam_train_losses, Adam_valid_losses, Adam_valid_accs, Adam_params = train_UNet(args, optimizer=Adam(alpha=0.01))"
      ],
      "metadata": {
        "id": "QaSRkNcKExfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn, Adam_Adam_train_losses, Adam_Adam_valid_losses, Adam_Adam_valid_accs, Adam_params = train_UNet(args, optimizer=Adam(alpha=0.01, optimizer=Adam(alpha=1e-5)))"
      ],
      "metadata": {
        "id": "0V0MBpXXE-Kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn, Adam_AMSGrad_train_losses, Adam_AMSGrad_valid_losses, Adam_AMSGrad_valid_accs, Adam_AMSGrad_params = train_UNet(args, optimizer=Adam(alpha=0.01,optimizer=(AMSGrad(alpha=1e-5))))"
      ],
      "metadata": {
        "id": "SUdVMapAE-r0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AdaBelief"
      ],
      "metadata": {
        "id": "0WCt0cYIQITr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn, AdaBelief_SGD_train_losses, AdaBelief_SGD_valid_losses, AdaBelief_SGD_valid_accs, AdaBelief_SGD_params = train_UNet(args, optimizer=AdaBelief(alpha=0.01,optimizer=SGD(1e-6)))"
      ],
      "metadata": {
        "id": "Oq01lWJiycej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "holh_MKyWJbS"
      },
      "outputs": [],
      "source": [
        "cnn, AdaBelief_train_losses, AdaBelief_valid_losses, AdaBelief_valid_accs, AdaBelief_params = train_UNet(args, optimizer=AdaBelief(alpha=0.01))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "breBK9XMWJzx"
      },
      "outputs": [],
      "source": [
        "cnn, AdaBelief_Adam_train_losses, AdaBelief_Adam_valid_losses, AdaBelief_Adam_valid_accs, AdaBelief_Adam_params = train_UNet(args, optimizer=AdaBelief(alpha=0.01,optimizer=Adam(alpha=1e-6)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3gDVI-OWKFu"
      },
      "outputs": [],
      "source": [
        "cnn, AdaBelief_AdaBelief_train_losses, AdaBelief_AdaBelief_valid_losses, AdaBelief_AdaBelief_valid_accs, AdaBelief_AdaBelief_params = train_UNet(args, optimizer=AdaBelief(0.01,optimizer=AdaBelief(alpha=1e-6)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3NP2eXa6p8C"
      },
      "outputs": [],
      "source": [
        "cnn, AdaBeliefBaydin_AdaBelief_train_losses, AdaBeliefBaydin_AdaBelief_valid_losses, AdaBeliefBaydin_AdaBelief_valid_accs, AdaBeliefBaydin_AdaBelief_params = train_UNet(args, optimizer=AdaBeliefBaydin(0.01,optimizer=AdaBelief(alpha=1e-6)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn, AdaBeliefBaydin_Adam_train_losses, AdaBeliefBaydin_Adaam_valid_losses, AdaBeliefBaydin_Adam_valid_accs, AdaBeliefBaydin_Adam_params = train_UNet(args, optimizer=AdaBeliefBaydin(0.01,optimizer=Adam(alpha=1e-6)))"
      ],
      "metadata": {
        "id": "ec7s_U6zTQKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AMSGrad"
      ],
      "metadata": {
        "id": "ciiY3YdRQMlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn, AMSGrad_SGD_train_losses, AMSGrad_SGD_valid_losses, AMSGrad_SGD_valid_accs, AMSGrad_SGD_params = train_UNet(args, optimizer=AMSGrad(alpha=0.02,optimizer=SGD(1e-5)))"
      ],
      "metadata": {
        "id": "ilJLfIQsytM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yhj2duaWKWq"
      },
      "outputs": [],
      "source": [
        "cnn, AMSGrad_train_losses, AMSGrad_valid_losses, AMSGrad_valid_accs, AMSGrad_params = train_UNet(args, optimizer=AMSGrad(alpha=0.02))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsFvggiAWKwb"
      },
      "outputs": [],
      "source": [
        "cnn, AMSGrad_Adam_train_losses, AMSGrad_Adam_valid_losses, AMSGrad_Adam_valid_accs, AMSGrad_Adam_params = train_UNet(args, optimizer=AMSGrad(0.02,optimizer=Adam(alpha=1e-4)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G5myUp6WOBA"
      },
      "outputs": [],
      "source": [
        "cnn, AMSGrad_AMSGrad_train_losses, AMSGrad_AMSGrad_valid_losses, AMSGrad_AMSGrad_valid_accs, AMSGrad_AMSGrad_params = train_UNet(args, optimizer=AMSGrad(0.02,optimizer=AMSGrad(alpha=1e-5)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4q7eA42Z64zD"
      },
      "outputs": [],
      "source": [
        "cnn, AMSGradBaydin_Adam_train_losses, AMSGradBaydin_Adam_valid_losses, AMSGradBaydin_Adam_valid_accs, AMSGradBaydin_Adam_params = train_UNet(args, optimizer=AMSGradBaydin(0.02,optimizer=Adam(1e-5)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lpvziCx7Bia"
      },
      "outputs": [],
      "source": [
        "cnn, AMSGradBaydin_AMSGrad_train_losses, AMSGradBaydin_AMSGrad_valid_losses, AMSGradBaydin_AMSGrad_valid_accs, AMSGradBaydin_AMSGrad_params = train_UNet(args, optimizer=AMSGradBaydin(0.02,optimizer=AMSGrad(1e-5)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot"
      ],
      "metadata": {
        "id": "OizdKrIIQcaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SGD"
      ],
      "metadata": {
        "id": "kz33PI8INiDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(8, 13))\n",
        "ax1.plot(SGD_train_losses, label='SGD')\n",
        "ax1.plot(SGD_Adam_train_losses, label='SGD with Adam hyperoptimizer')\n",
        "ax1.plot(SGD_AMSGrad_train_losses, label='SGD with AMSGrad tuned hyperparameters')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "ax2.plot(SGD_valid_losses, label='SGD')\n",
        "ax2.plot(SGD_Adam_valid_losses, label='SGD with Adam hyperoptimizer')\n",
        "ax2.plot(SGD_AMSGrad_valid_losses, label='SGD with AMSGrad tuned hyperoptimizer')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "\n",
        "ax3.plot(SGD_valid_accs, label='SGD')\n",
        "ax3.plot(SGD_Adam_valid_accs, label='SGD with Adam hyperoptimizer')\n",
        "ax3.plot(SGD_AMSGrad_valid_accs, label='SGD with AMSGrad hyperoptimizer')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.set_title('Validation Accuracy')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YoRczGdfNkNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adam"
      ],
      "metadata": {
        "id": "RVGWy09uG2Am"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(8, 13))\n",
        "# Plot training loss on the top\n",
        "ax1.plot(Adam_train_losses, label='Adam')\n",
        "ax1.plot(Adam_Adam_train_losses, label='Adam with Adam hyperoptimizer')\n",
        "ax1.plot(Adam_AMSGrad_train_losses, label='Adam with AMSGrad hyperoptimizer')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "# Plot validation loss on the bottom\n",
        "ax2.plot(Adam_valid_losses, label='Adam')\n",
        "ax2.plot(Adam_Adam_valid_losses, label='Adam with Adam hyperoptimizer')\n",
        "ax2.plot(Adam_AMSGrad_valid_losses, label='Adam with AMSGrad hyperoptimizer')\n",
        "\n",
        "\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "ax3.plot(Adam_valid_accs, label='Adam')\n",
        "ax3.plot(Adam_Adam_valid_accs, label='Adam with Adam hyperoptimizer')\n",
        "ax3.plot(Adam_AMSGrad_valid_accs, label='Adam with AMSGrad hyperoptimizer')\n",
        "\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.set_title('Validation Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6iJ_JfYAG38A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTXqVabXkBBa"
      },
      "source": [
        "#### AdaBelief"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S9XIZbijpkY"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(8, 13))\n",
        "# Plot training loss on the top\n",
        "ax1.plot(AdaBelief_train_losses, label='AdaBelief')\n",
        "ax1.plot(AdaBeliefBaydin_AdaBelief_train_losses, label='AdaBelief with AdaBelief hyperoptimizer (learning rate only)')\n",
        "\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "# Plot validation loss on the bottom\n",
        "ax2.plot(AdaBelief_valid_losses, label='AdaBelief')\n",
        "ax2.plot(AdaBeliefBaydin_AdaBelief_valid_losses, label='AdaBelief with AdaBelief hyperoptimizer (learning rate only)')\n",
        "\n",
        "\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "\n",
        "ax3.plot(AdaBelief_valid_accs, label='AdaBelief')\n",
        "ax3.plot(AdaBeliefBaydin_AdaBelief_valid_accs, label='AdaBelief with AdaBelief hyperoptimizer (learning rate only)')\n",
        "\n",
        "\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.set_title('Validation Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(8, 13))\n",
        "# Plot training loss on the top\n",
        "ax1.plot(AdaBelief_train_losses, label='AdaBelief')\n",
        "ax1.plot(AdaBelief_Adam_train_losses, label='AdaBelief with Adam hyperoptimizer')\n",
        "ax1.plot(AdaBelief_AdaBelief_train_losses, label='AdaBelief with AdaBelief hyperoptimizer')\n",
        "\n",
        "ax1.plot(AdaBeliefBaydin_Adam_train_losses, label='AdaBelief with Adam hyperoptimizer (learning rate only)')\n",
        "ax1.plot(AdaBeliefBaydin_AdaBelief_train_losses, label='AdaBelief with AdaBelief hyperoptimizer (learning rate only)')\n",
        "\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "# Plot validation loss on the bottom\n",
        "ax2.plot(AdaBelief_valid_losses, label='AdaBelief')\n",
        "ax2.plot(AdaBelief_Adam_valid_losses, label='AdaBelief with Adam hyperoptimizer')\n",
        "ax2.plot(AdaBelief_AdaBelief_valid_losses, label='AdaBelief with AdaBelief hyperoptimizer')\n",
        "ax2.plot(AdaBeliefBaydin_Adam_valid_losses, label='AdaBelief with Adam hyperoptimizer (learning rate only)')\n",
        "ax2.plot(AdaBeliefBaydin_AdaBelief_valid_losses, label='AdaBelief with AdaBelief hyperoptimizer (learning rate only)')\n",
        "\n",
        "\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "\n",
        "ax3.plot(AdaBelief_valid_accs, label='AdaBelief')\n",
        "ax3.plot(AdaBelief_Adam_valid_accs, label='AdaBelief with Adam hyperoptimizer')\n",
        "ax3.plot(AdaBelief_AdaBelief_valid_accs, label='AdaBelief with AdaBelief hyperoptimizer')\n",
        "ax3.plot(AdaBeliefBaydin_Adam_valid_accs, label='AdaBelief with Adam hyperoptimizer (learning rate only)')\n",
        "ax3.plot(AdaBeliefBaydin_AdaBelief_valid_accs, label='AdaBelief with AdaBelief hyperoptimizer (learning rate only)')\n",
        "\n",
        "\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.set_title('Validation Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zXVLyNt2Db_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AMSGrad"
      ],
      "metadata": {
        "id": "_VICoqs5QxEg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zytLdozlIRT"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(8, 13))\n",
        "\n",
        "# Plot training loss on the top\n",
        "ax1.plot(AMSGrad_train_losses, label='AMSGrad')\n",
        "ax1.plot(AMSGrad_Adam_train_losses, label='AMSGrad with Adam hyperoptimizer')\n",
        "ax1.plot(AMSGrad_AMSGrad_train_losses, label='AMSGrad with AMSGrad hyperoptimizer')\n",
        "ax1.plot(AMSGradBaydin_Adam_train_losses, label='AMSGrad with Adam hyperoptimizer (learning rate only)')\n",
        "ax1.plot(AMSGradBaydin_AMSGrad_train_losses, label='AMSGrad with AMSGrad hyperoptimizer (learning rate only)')\n",
        "\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "# Plot validation loss on the bottom\n",
        "ax2.plot(AMSGrad_valid_losses, label='AMSGrad')\n",
        "ax2.plot(AMSGrad_Adam_valid_losses, label='AMSGrad with Adam hyperoptimizer')\n",
        "ax2.plot(AMSGrad_AMSGrad_valid_losses, label='AMSGrad with AMSGrad hyperoptimizer')\n",
        "ax2.plot(AMSGradBaydin_Adam_valid_losses, label='AMSGrad with Adam hyperoptimizer (learning rate only)')\n",
        "ax2.plot(AMSGradBaydin_AMSGrad_valid_losses, label='AMSGrad with AMSGrad hyperoptimizer (learning rate only)')\n",
        "\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "\n",
        "ax3.plot(AMSGrad_valid_accs, label='AMSGrad')\n",
        "ax3.plot(AMSGrad_Adam_valid_accs, label='AMSGrad with Adam hyperoptimizer')\n",
        "ax3.plot(AMSGrad_AMSGrad_valid_accs, label='AMSGrad with AMSGrad hyperoptimizer')\n",
        "ax3.plot(AMSGradBaydin_Adam_valid_accs, label='AMSGrad with Adam hyperoptimizer (learning rate only)')\n",
        "ax3.plot(AMSGradBaydin_AMSGrad_valid_accs, label='AMSGrad with AMSGrad hyperoptimizer (learning rate only)')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.set_title('Validation Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(8, 13))\n",
        "\n",
        "# Plot training loss on the top\n",
        "ax1.plot(AMSGrad_train_losses, label='AMSGrad')\n",
        "ax1.plot(AMSGrad_AMSGrad_train_losses, label='AMSGrad with AMSGrad hyperoptimizer')\n",
        "\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "# Plot validation loss on the bottom\n",
        "ax2.plot(AMSGrad_valid_losses, label='AMSGrad')\n",
        "\n",
        "ax2.plot(AMSGrad_AMSGrad_valid_losses, label='AMSGrad with AMSGrad hyperoptimizer')\n",
        "\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "\n",
        "ax3.plot(AMSGrad_valid_accs, label='AMSGrad')\n",
        "ax3.plot(AMSGrad_AMSGrad_valid_accs, label='AMSGrad with AMSGrad hyperoptimizer')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.set_title('Validation Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LLTRJeLbB9OG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Overall Plot"
      ],
      "metadata": {
        "id": "k2oL1lH2isEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(8, 17))\n",
        "\n",
        "# Plot training loss on the top\n",
        "ax1.plot(AMSGrad_train_losses, label='AMSGrad')\n",
        "ax1.plot(AMSGrad_Adam_train_losses, label='AMSGrad with Adam hyperoptimizer')\n",
        "ax1.plot(AMSGrad_AMSGrad_train_losses, label='AMSGrad with AMSGrad hyperoptimizer')\n",
        "ax1.plot(AMSGradBaydin_Adam_train_losses, label='AMSGrad with Adam hyperoptimizer (learning rate only)')\n",
        "ax1.plot(AMSGradBaydin_AMSGrad_train_losses, label='AMSGrad with AMSGrad hyperoptimizer (learning rate only)')\n",
        "ax1.plot(AdaBelief_train_losses, label='AdaBelief')\n",
        "ax1.plot(AdaBelief_Adam_train_losses, label='AdaBelief with Adam hyperoptimizer')\n",
        "ax1.plot(AdaBelief_AdaBelief_train_losses, label='AdaBelief with AdaBelief hyperoptimizer')\n",
        "\n",
        "ax1.plot(AdaBeliefBaydin_Adam_train_losses, label='AdaBelief with Adam hyperoptimizer (learning rate only)')\n",
        "ax1.plot(AdaBeliefBaydin_AdaBelief_train_losses, label='AdaBelief with AdaBelief hyperoptimizer (learning rate only)')\n",
        "\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "# Plot validation loss on the bottom\n",
        "ax2.plot(AMSGrad_valid_losses, label='AMSGrad')\n",
        "ax2.plot(AMSGrad_Adam_valid_losses, label='AMSGrad with Adam hyperoptimizer')\n",
        "ax2.plot(AMSGrad_AMSGrad_valid_losses, label='AMSGrad with AMSGrad hyperoptimizer')\n",
        "ax2.plot(AMSGradBaydin_Adam_valid_losses, label='AMSGrad with Adam hyperoptimizer (learning rate only)')\n",
        "ax2.plot(AMSGradBaydin_AMSGrad_valid_losses, label='AMSGrad with AMSGrad hyperoptimizer (learning rate only)')\n",
        "ax2.plot(AdaBelief_valid_losses, label='AdaBelief')\n",
        "ax2.plot(AdaBelief_Adam_valid_losses, label='AdaBelief with Adam hyperoptimizer')\n",
        "ax2.plot(AdaBelief_AdaBelief_valid_losses, label='AdaBelief with AdaBelief hyperoptimizer')\n",
        "ax2.plot(AdaBeliefBaydin_Adam_valid_losses, label='AdaBelief with Adam hyperoptimizer (learning rate only)')\n",
        "ax2.plot(AdaBeliefBaydin_AdaBelief_valid_losses, label='AdaBelief with AdaBelief hyperoptimizer (learning rate only)')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "\n",
        "ax3.plot(AMSGrad_valid_accs, label='AMSGrad')\n",
        "ax3.plot(AMSGrad_Adam_valid_accs, label='AMSGrad with Adam hyperoptimizer')\n",
        "ax3.plot(AMSGrad_AMSGrad_valid_accs, label='AMSGrad with AMSGrad hyperoptimizer')\n",
        "ax3.plot(AMSGradBaydin_Adam_valid_accs, label='AMSGrad with Adam hyperoptimizer (learning rate only)')\n",
        "ax3.plot(AMSGradBaydin_AMSGrad_valid_accs, label='AMSGrad with AMSGrad hyperoptimizer (learning rate only)')\n",
        "ax3.plot(AdaBelief_valid_accs, label='AdaBelief')\n",
        "ax3.plot(AdaBelief_Adam_valid_accs, label='AdaBelief with Adam hyperoptimizer')\n",
        "ax3.plot(AdaBelief_AdaBelief_valid_accs, label='AdaBelief with AdaBelief hyperoptimizer')\n",
        "ax3.plot(AdaBeliefBaydin_Adam_valid_accs, label='AdaBelief with Adam hyperoptimizer (learning rate only)')\n",
        "ax3.plot(AdaBeliefBaydin_AdaBelief_valid_accs, label='AdaBelief with AdaBelief hyperoptimizer (learning rate only)')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.set_title('Validation Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bLDCubIohm1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall Plot with only best hyperoptimizers"
      ],
      "metadata": {
        "id": "qaGobkOEkXAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(8, 17))\n",
        "\n",
        "# Plot training loss on the top\n",
        "ax1.plot(SGD_train_losses, label='SGD')\n",
        "ax1.plot(SGD_Adam_train_losses, label='SGD with Adam hyperoptimizer')\n",
        "ax1.plot(Adam_AMSGrad_train_losses, label='Adam with AMSGrad hyperoptimizer (learning rate only)')\n",
        "ax1.plot(Adam_Adam_train_losses, label='Adam')\n",
        "ax1.plot(AdaBelief_train_losses, label='AdaBelief')\n",
        "ax1.plot(AdaBeliefBaydin_AdaBelief_train_losses, label='AdaBelief with AdaBelief hyperoptimizer (learning rate only)')\n",
        "ax1.plot(AMSGrad_train_losses, label='AMSGrad hyperoptimizer')\n",
        "ax1.plot(AMSGrad_AMSGrad_train_losses, label='AMSGrad with AMSGrad hyperoptimizer')\n",
        "\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "# Plot validation loss on the bottom\n",
        "ax2.plot(SGD_valid_losses, label='SGD')\n",
        "ax2.plot(SGD_Adam_valid_losses, label='SGD with Adam hyperoptimizer')\n",
        "ax2.plot(Adam_AMSGrad_valid_losses, label='Adam with AMSGrad hyperoptimizer (learning rate only)')\n",
        "ax2.plot(Adam_Adam_valid_losses, label='Adam')\n",
        "ax2.plot(AdaBelief_valid_losses, label='AdaBelief')\n",
        "ax2.plot(AdaBeliefBaydin_AdaBelief_valid_losses, label='AdaBelief with AdaBelief hyperoptimizer (learning rate only)')\n",
        "ax2.plot(AMSGrad_valid_losses, label='AMSGrad hyperoptimizer')\n",
        "ax2.plot(AMSGrad_AMSGrad_valid_losses, label='AMSGrad with AMSGrad hyperoptimizer')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "\n",
        "\n",
        "ax3.plot(SGD_valid_accs, label='SGD')\n",
        "ax3.plot(SGD_Adam_valid_accs, label='SGD with Adam hyperoptimizer')\n",
        "ax3.plot(Adam_AMSGrad_valid_accs, label='Adam with AMSGrad hyperoptimizer (learning rate only)')\n",
        "ax3.plot(Adam_Adam_valid_accs, label='Adam')\n",
        "ax3.plot(AdaBelief_valid_accs, label='AdaBelief')\n",
        "ax3.plot(AdaBeliefBaydin_AdaBelief_valid_accs, label='AdaBelief with AdaBelief hyperoptimizer (learning rate only)')\n",
        "ax3.plot(AMSGrad_valid_accs, label='AMSGrad hyperoptimizer')\n",
        "ax3.plot(AMSGrad_AMSGrad_valid_accs, label='AMSGrad with AMSGrad hyperoptimizer')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.set_title('Validation Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fMk-ueV3kaSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7du0WCpv96s"
      },
      "source": [
        "### Test Optimizers with hyperoptimized hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Function"
      ],
      "metadata": {
        "id": "RR_asff0R1xn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Llo3YQum0NTw"
      },
      "outputs": [],
      "source": [
        "def train_UNet_general_optimizer(args, cnn=None,optimizer=None):\n",
        "    torch.set_num_threads(5)\n",
        "    npr.seed(args.seed)\n",
        "    save_dir = \"outputs/\" + args.experiment_name\n",
        "    colours = np.load(args.colours, allow_pickle=True, encoding=\"bytes\")[0]\n",
        "    num_colours = np.shape(colours)[0]\n",
        "    num_in_channels = 1 if not args.downsize_input else 3\n",
        "    if cnn is None:\n",
        "        Net = globals()[args.model]\n",
        "        cnn = Net(args.kernel, args.num_filters, num_colours, num_in_channels)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    print(\"Loading data...\")\n",
        "    (x_train, y_train), (x_test, y_test) = load_cifar10()\n",
        "\n",
        "    print(\"Transforming data...\")\n",
        "    train_rgb, train_grey = process(x_train, y_train, downsize_input=args.downsize_input)\n",
        "    train_rgb_cat = get_rgb_cat(train_rgb, colours)\n",
        "    test_rgb, test_grey = process(x_test, y_test, downsize_input=args.downsize_input)\n",
        "    test_rgb_cat = get_rgb_cat(test_rgb, colours)\n",
        "\n",
        "    print(\"Beginning training ...\")\n",
        "    if args.gpu:\n",
        "        cnn.cuda()\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    valid_accs = []\n",
        "    total_time = 0\n",
        "    start_time = time.time()\n",
        "    mw = ModuleWrapper(cnn, optimizer=optimizer)\n",
        "    mw.initialize()\n",
        "    \n",
        "    for epoch in range(args.epochs):\n",
        "        start = time.time()\n",
        "        # Train the Model\n",
        "        cnn.train()  # Change model to 'train' mode\n",
        "        losses = []\n",
        "        \n",
        "        for i, (xs, ys) in enumerate(get_batch(train_grey, train_rgb_cat, args.batch_size)):\n",
        "            mw.begin()\n",
        "            images, labels = get_torch_vars(xs, ys, args.gpu)\n",
        "            pred = mw.forward(images)\n",
        "            # Forward + Backward + Optimize\n",
        "\n",
        "            loss = compute_loss(\n",
        "                criterion, pred, labels, batch_size=args.batch_size, num_colours=num_colours\n",
        "            )\n",
        "            mw.zero_grad()\n",
        "            loss.backward(create_graph=True)\n",
        "            mw.step()\n",
        "            losses.append(loss.data.item())\n",
        "        time_elapsed = time.time() - start\n",
        "        avg_loss = np.mean(losses)\n",
        "        train_losses.append(avg_loss)\n",
        "        \n",
        "        print(\n",
        "            \"Epoch [%d/%d], Loss: %.4f, Time (s): %d\"\n",
        "            % (epoch + 1, args.epochs, avg_loss, time_elapsed)\n",
        "        )\n",
        "        total_time += time_elapsed\n",
        "\n",
        "        # Evaluate the model\n",
        "        cnn.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
        "        val_loss, val_acc = run_validation_step(\n",
        "            cnn,\n",
        "            criterion,\n",
        "            test_grey,\n",
        "            test_rgb_cat,\n",
        "            args.batch_size,\n",
        "            colours,\n",
        "            save_dir + \"/test_%d.png\" % epoch,\n",
        "            args.visualize,\n",
        "            args.downsize_input,\n",
        "        )\n",
        "\n",
        "        valid_losses.append(val_loss)\n",
        "        valid_accs.append(val_acc.item())\n",
        "        print(\n",
        "            \"Epoch [%d/%d], Val Loss: %.4f, Val Acc: %.1f%%\"\n",
        "            % (epoch + 1, args.epochs, val_loss, val_acc)\n",
        "        )\n",
        "    print('Total training time: {:.2f} seconds'.format(total_time))\n",
        "    final_parameters={}\n",
        "\n",
        "    return cnn, train_losses, valid_losses, valid_accs, final_parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SGD"
      ],
      "metadata": {
        "id": "MErGJRQqRXSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optimizer = SGD(alpha= 0.008516764268279076)\n",
        "cnn, Hyperoptimized_SGD_train_losses, Hyperoptimized_SGD_valid_losses, Hyperoptimized_SGD_valid_accs, _ = train_UNet_general_optimizer(args, cnn=None,optimizer=optimizer)\n",
        "\n"
      ],
      "metadata": {
        "id": "pk__lAA6RloW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(8, 13))\n",
        "ax1.plot(SGD_train_losses, label='SGD')\n",
        "ax1.plot(Hyperoptimized_SGD_train_losses, label='SGD with tuned hyperparameters')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "ax2.plot(SGD_valid_losses, label='SGD')\n",
        "ax2.plot(Hyperoptimized_SGD_valid_losses, label='SGD with tuned hyperparameters')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "\n",
        "ax3.plot(SGD_valid_accs, label='SGD')\n",
        "ax3.plot(Hyperoptimized_SGD_valid_accs, label='SGD with tuned hyperparameters')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.set_title('Validation Accuracy')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EnucIImPThpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adam\n",
        "\n"
      ],
      "metadata": {
        "id": "tN4ynva0RYgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optimizer = Adam(alpha= 0.000535776955075562, beta1= 0.8984992504119873, beta2= 0.9990102648735046)\n",
        "cnn, Hyperoptimized_Adam_train_losses, Hyperoptimized_Adam_valid_losses, Hyperoptimized_Adam_valid_accs, _ = train_UNet_general_optimizer(args, cnn=None,optimizer=optimizer)\n",
        "\n"
      ],
      "metadata": {
        "id": "x-6s673cRkme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(8, 13))\n",
        "ax1.plot(Adam_train_losses, label='Adam')\n",
        "ax1.plot(Hyperoptimized_Adam_train_losses, label='Adam with tuned hyperparameters')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "ax2.plot(Adam_valid_losses, label='Adam')\n",
        "ax2.plot(Hyperoptimized_Adam_valid_losses, label='Adam with tuned hyperparameters')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "\n",
        "ax3.plot(Adam_valid_accs, label='Adam')\n",
        "ax3.plot(Hyperoptimized_Adam_valid_accs, label='Adam with tuned hyperparameters')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.set_title('Validation Accuracy')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S1fssxGwTKnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AdaBelief"
      ],
      "metadata": {
        "id": "njE8rJhuR9fm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ozpJjJGv-wE"
      },
      "outputs": [],
      "source": [
        "\n",
        "optimizer = AdaBelief(alpha= 0.00835033692419529)\n",
        "cnn, Hyperoptimized_AdaBelief_train_losses, Hyperoptimized_AdaBelief_valid_losses, Hyperoptimized_AdaBelief_valid_accs, _ = train_UNet_general_optimizer(args, cnn=None,optimizer=optimizer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(8, 13))\n",
        "ax1.plot(AdaBelief_train_losses, label='AdaBelief')\n",
        "ax1.plot(Hyperoptimized_AdaBelief_train_losses, label='AdaBelief with tuned hyperparameters')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "ax2.plot(AdaBelief_valid_losses, label='AdaBelief')\n",
        "ax2.plot(Hyperoptimized_AdaBelief_valid_losses, label='AdaBelief with tuned hyperparameters')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "\n",
        "ax3.plot(AdaBelief_valid_accs, label='AdaBelief')\n",
        "ax3.plot(Hyperoptimized_AdaBelief_valid_accs, label='AdaBelief with tuned hyperparameters')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.set_title('Validation Accuracy')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8lwiC10Ur_2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AMSGrad"
      ],
      "metadata": {
        "id": "Zr66Y1z0R_o0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCFUbWr-zRVA"
      },
      "outputs": [],
      "source": [
        "optimizer = AMSGrad(alpha= 0.013616234995424747, beta1= 0.8991430997848511, beta2=0.9989991784095764)\n",
        "cnn, Hyperoptimized_AMSGrad_train_losses, Hyperoptimized_AMSGrad_valid_losses, Hyperoptimized_AMSGrad_valid_accs, Hyperoptimized_AMSGrad_final_parameters = train_UNet_general_optimizer(args, cnn=None,optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(8, 13))\n",
        "ax1.plot(AdaBelief_train_losses, label='AMSGrad')\n",
        "ax1.plot(Hyperoptimized_AMSGrad_train_losses, label='AMSGrad with tuned hyperparameters')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Training Loss')\n",
        "\n",
        "ax2.plot(AMSGrad_valid_losses, label='AMSGrad')\n",
        "ax2.plot(Hyperoptimized_AMSGrad_valid_losses, label='AMSGrad with tuned hyperparameters')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_title('Validation Loss')\n",
        "\n",
        "ax3.plot(AMSGrad_valid_accs, label='AMSGrad')\n",
        "ax3.plot(Hyperoptimized_AMSGrad_valid_accs, label='AMSGrad with tuned hyperparameters')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend()\n",
        "ax3.set_title('Validation Accuracy')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QGc5zYeAoiDW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9NE_CRp72_nR",
        "NK4ghAG00cCT",
        "Iti2Tnjxxwqn",
        "9xWEov2N3I-f",
        "_r6e6jBC3NgF",
        "7aoAhXhVCLBN",
        "iOJt4xjfC6Pn",
        "Hkz_F_FlDQ-S",
        "zN3FF8hIlS7h",
        "Ft0qRJgWlK2q",
        "rp_wCpMjqt5w",
        "Rb7Agthjf9qs"
      ],
      "provenance": [],
      "toc_visible": true,
      "gpuType": "A100"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}